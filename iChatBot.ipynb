{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.ops.rnn_cell import GRUCell\n",
    "from tensorflow.python.ops.rnn_cell import LSTMCell\n",
    "from tensorflow.python.ops.rnn_cell import MultiRNNCell\n",
    "from tensorflow.python.ops.rnn_cell import DropoutWrapper, ResidualWrapper\n",
    "\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.util import nest\n",
    "\n",
    "from tensorflow.contrib.seq2seq.python.ops import attention_wrapper\n",
    "from tensorflow.contrib.seq2seq.python.ops import beam_search_decoder\n",
    "\n",
    "from preprocess import *\n",
    "from loading_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resetter\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding parameters\n",
    "embedding_size = 50\n",
    "vocab_size = 400003\n",
    "\n",
    "#data parameters\n",
    "eMax_allowed_length = 20\n",
    "dMax_allowed_length = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching data\n",
    "#default directory: 'data/data_10.csv'\n",
    "X,Y= read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching glove vectors\n",
    "#default directory: \"./glove.6B.50d.txt\"\n",
    "embedding_size = 50\n",
    "wi,iw,wv = read_glove_vecs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding extra tokens to glove dictionary\n",
    "go_index,eos_index,unk_index = add_extra_to_dict(wi,iw,wv,embedding_size)\n",
    "emb = map_dict_to_list(iw,wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing data\n",
    "#Mapping each word in a sentence to its glove index\n",
    "eInput,eLengths = fit_encoder_text(data= X[1:],word_to_index = wi,max_allowed_seq_length = eMax_allowed_length)\n",
    "dInput,dOutput,dLengths = fit_decoder_text(data= Y[1:],word_to_index = wi,max_allowed_seq_length = dMax_allowed_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel():\n",
    "    def __init__(self,config,mode):\n",
    "        assert mode.lower() in ['train','decode']\n",
    "        \n",
    "        self.mode = mode.lower()\n",
    "        \n",
    "        #num_encoder_symbols and num_decoder_symbols\n",
    "        self.encoder_vocab_size = 50\n",
    "        self.decoder_vocab_size = 50\n",
    "\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        \n",
    "        self.cell_type = config['cell_type']\n",
    "        self.hidden_units = config['hidden_units']\n",
    "        self.depth = config['depth']\n",
    "        self.attention_type = config['attention_type']\n",
    "        self.embedding_size = config['embedding_size']\n",
    "        \n",
    "        self.use_residual = config['use_residual']\n",
    "        self.attn_input_feeding = config['attn_input_feeding']\n",
    "        self.use_dropout = config['use_dropout']\n",
    "        self.keep_prob = 1.0 - config['dropout_rate']\n",
    "        \n",
    "        self.optimizer = config['optimizer']\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.max_gradient_norm = config['max_gradient_norm']\n",
    "        self.global_step = tf.Variable(0, trainable = Flase, name = 'global_step')\n",
    "        self.global_epoch_step = tf.Variable(0,trainable=False, name = \"global_epoch_step\")\n",
    "        self.global_epoch_step_op= tf.assign(self.global_epoch_step,self.global_epoch_step+1)\n",
    "        \n",
    "        self.dtype = tf.float16 if config['use_float16'] else tf.float32\n",
    "        self.keep_prob_placeholder = tf.placeholder(self.dtype, shape=[], name = 'keep_prob')\n",
    "        \n",
    "        self.use_beamsearch_decode = False\n",
    "        if self.mode == 'decode':\n",
    "            self.beam_width = config['beam_width']\n",
    "            self.use_beamsearch_decode = True if self.beam_width > 1 else False\n",
    "            self.max_decode_step = config['max_decode_step']\n",
    "        \n",
    "        self.build_model()\n",
    "    \n",
    "    def build_model(self):\n",
    "            print('building model..')\n",
    "\n",
    "            #building encoder and decoder networks\n",
    "            self.init_placeholders()\n",
    "            '''\n",
    "            self.build_encoder()\n",
    "            self.build_decoder()\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "            '''    \n",
    "    def init_placeholders(self):\n",
    "            #encoder inputs: [batch_size, max_time_steps]\n",
    "            self.encoder_inputs = tf.placeholder(dtype = tf.int32, shape = (None,None), name = 'encoder_inputs')\n",
    "            #encoder_inputs_length: [batch_size]\n",
    "            self.encoder_inputs_length = tf.placeholder(dtype=tf.int32, shape=(None,) , name = 'encoder_inputs_length')\n",
    "            \n",
    "            #get dynamic batch_size\n",
    "            self.batch_size = tf.shape(self.encoder_inputs)[0]\n",
    "            \n",
    "            if(self.mode=='train'):\n",
    "                \n",
    "                #decoder_inputs: [batch_size,max_time_steps]\n",
    "                self.decoder_inputs = tf.placeholder(dtype=tf.int32,shape=(None,None), name ='decoder_inputs')\n",
    "                #decoder_inputs_length: [batch_size]\n",
    "                self.decoder_inputs_length = tf.placeholder(dtype=tf.int32, shape=(None,), name='decoder_inputs_length')\n",
    "                \n",
    "                self.decoder_targets = tf.placeholder(dtype=tf.int32,shape=(None,None), name ='decoder_targets')\n",
    "                \n",
    "                '''\n",
    "                #No need, already preprocessed\n",
    "                decoder_start_token=tf.ones(shape=[self.batch_size,1], dtype=tf.int32)*data_utils.start_token\n",
    "                \n",
    "                decoder_end_token=tf.ones(shape=[self.batch_size,1], dtype=tf.int32)*data_utils.end_token\n",
    "                '''\n",
    "                \n",
    "    def build_single_cell(self):\n",
    "        cell_type = LSTMCell\n",
    "        if(self.cell_type.lower() == 'gru'):\n",
    "            cell_type = GRUCell\n",
    "        cell = cell_type(self.hidden_units)\n",
    "        \n",
    "        if self.use_dropout:\n",
    "            cell = DropoutWrapper(cell,dtype=self.dtype,\n",
    "                                 output_keep_prob = self.keep_prob_placeholder)\n",
    "            \n",
    "        if self.use_resideual:\n",
    "            cell = ResidualWrapper(cell)\n",
    "            \n",
    "        return cell\n",
    "\n",
    "    def build_encoder_cell (self):\n",
    "        return MultiRNNCell([self.build_single_cell() for i in range(self.depth)])\n",
    "    \n",
    "    def build_decoder_cell(self):\n",
    "        encoder_outputs = self.encoder_outputs\n",
    "        encoder_last_state = self.encoder_last_state\n",
    "        encoder_inputs_length = self.encoder_inputs_length\n",
    "        \n",
    "        if self.use_beamsearch_decode:\n",
    "            print('using beamsearch..')\n",
    "            encoder_outputs = seq2seq.tile_batch(self.encoder_outputs,\n",
    "                                                 multiplier=self.beam_width)\n",
    "            encoder_last_state = nest.map_structure( lambda s: seq2seq.tile_batch(s,self.beam_width),\n",
    "                                                   self.encoder_last_state)\n",
    "            encoder_inputs_length = seq2seq.tile_batch(self.encoder_inputs_length,\n",
    "                                                       multiplier=self.beam_width)\n",
    "            #Building attention mechanism: Default Bahdanau\n",
    "            #'Bahdanau' style attention\n",
    "            self.attention_mechanism = attention_wrapper.BahdanauAttention(\n",
    "            num_units=self.hidden_units, memory=encoder_outputs,\n",
    "                memory_sequence_length=encoder_inputs_length,\n",
    "                name='BahdanauAttention'\n",
    "            )\n",
    "            # 'Luong' style attention:\n",
    "            if self.attention_type.lower() == 'luong':\n",
    "                self.attention_mechanism = attention_wrapper.LuongAttention(\n",
    "                num_units = self.hidden_units, memory=encoder_outputs,\n",
    "                memory_sequence_length=encoder_inputs_length,\n",
    "                name='LuongAttention')\n",
    "                \n",
    "            #Building decoder_cell\n",
    "            self.decoder_cell_list = [self.build_single_cell() for i in range(self.depth)]\n",
    "        decoder_initial_state = encoder_last_state\n",
    "        \n",
    "        def attn_decoder_input_fn(inputs,attention):\n",
    "            if not self.attn_input_feeding:\n",
    "                return input\n",
    "            \n",
    "            _input_layer = Dense(self.hidden_units,dtype = self.dtype,\n",
    "                                name = 'attn_input_feeding')\n",
    "            return _input_layer(array_ops.concat([inputs,attention],-1))\n",
    "        \n",
    "        self.decoder_cell_list[-1] = attention_wrapper.AttentionWrapper(\n",
    "        cell = self.decoder_cell_list[-1],\n",
    "        attention_mechanism=self.attention_mechanism,\n",
    "        attention_layer_size=self.hidden_units\n",
    "        cell_input_fn=attn_decoder_input_fn,\n",
    "        initial_cell_state=encoder_last_state[-1],\n",
    "        alignment_history=False,\n",
    "        name='Attention Wrapper')\n",
    "        \n",
    "        # Encoder last state must be compatible with AttentionWrapper\n",
    "        #Attentionwrapper.zero_state is used for the purpose\n",
    "        \n",
    "        batch_size = self.batch_size if not self.use_beamsearch_decode else self.batch_size*self.beam_width\n",
    "        initial_state = [state for state in encoder_last_state]\n",
    "        \n",
    "        initial_state[-1]= self.decoder_cell_list[-1].zero_state(\n",
    "        batch_size = batch_size, dtype=self.dtype)\n",
    "        decoder_initial_state = tuple(initial_state)\n",
    "        \n",
    "        return MultiRNNCell(self.decoder_cell_list),decoder_initial_state\n",
    "        \n",
    "    \n",
    "    def init_optimizer(self):\n",
    "        print(\"Setting optimizer..\")\n",
    "        #Gradients and SGD update operaton for training the model\n",
    "        trainable_params = tf.trainable_variables()\n",
    "        if self.optimizer.lower() == 'adadelta':\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate = self.learning_rate)\n",
    "        elif self.optmizer.lower() == 'adam':\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate = self.learning_rate)\n",
    "        elif self.optimizer.lower() == 'rmsprop':\n",
    "            self.opt = tf.train.RMSPropOptimizer(learning_rate = self.learning_rate)\n",
    "        else:\n",
    "            self.opt = tf.train.GradientDescentOptimizer(learning_rate = self.learning_rate)\n",
    "            \n",
    "        \n",
    "        #Compute gradients of loss w.r.t all trainable variables\n",
    "        gradients = tf.gradients(self.loss,trainable_params)\n",
    "        \n",
    "        #Clip gradients of loss w.r.t all trainable variables\n",
    "        clip_gradients,_ = tf.clip_by_global_norm(gradients,self.max_gradient_norm)\n",
    "        \n",
    "        #Update the model\n",
    "        self.updates = self.opt.apply_gradients(zip(clip_gradients,trainable_params),\n",
    "                                                global_step = self.global_step)\n",
    "        \n",
    "    def save(self,sess,path,var_list=None,global_step=None):\n",
    "        saver = tf.train.Saver(var_list)\n",
    "        \n",
    "        save_path = saver.save(sess,save_path=path,global_step=step)\n",
    "        print('model saved at ',save_path)\n",
    "        \n",
    "    def restore(self,sess,path,var_list=None)\n",
    "    saver = tf.train.Saver(var_list)\n",
    "    saver.restore(sess, save_path = path)\n",
    "    print('model restored from ',path)\n",
    "    \n",
    "    def build_encoder(self):\n",
    "        print('Building Encoder..')\n",
    "        with tf.variable_scope('encoder'):\n",
    "            self.encoder_cell = self.build_encoder_cell()\n",
    "            \n",
    "            #Instantiating pretrained embeddings\n",
    "            embedding_variable = tf.Variable(tf.constant(0.0, shape = [self.encoder_vocab_size, embedding_size]),trainable = False, name = 'embedding')\n",
    "                           \n",
    "            self.encoder_embedding_placeholder = tf.placeholder(tf.float32, shape=[self.encoder_vocab_size,embedding_size], name = 'embedding_placeholder' )\n",
    "            self.encoder_embeddings = embedding_variable.assign(self.encoder_embedding_placeholder)\n",
    "            self.encoder_inputs_embedded=tf.nn.embedding_lookup(self.encoder_embeddings,self.encoder_inputs)\n",
    "            \n",
    "    \n",
    "            #instantiating dense layer\n",
    "            input_layer = Dense(self.hidden_units, dtype = self.dtype, name = 'input_projection')\n",
    "            #passing the embedding through dense layer\n",
    "            self.encoder_input_embedded = input_layer(self.encoder_inputs_embedded)\n",
    "            \n",
    "            #Encode input sequences into context vectors\n",
    "            #encoder_outputs: [batch_size, max_time_step, cell_output_size]\n",
    "            #encoder_state: [batch_size,cell_output_size]\n",
    "            self.encoder_outputs, self.encoder_last_state = tf.nn.dynamic_rnn(cell = self.encoder_cell,\n",
    "                                                                               inputs=self.encoder_inputs_embedded,\n",
    "                                                                               sequence_length=self.encoder_inputs_length,\n",
    "                                                                               dtype=self.dtype,\n",
    "                                                                               time_major=False)\n",
    "            \n",
    "            '''\n",
    "            init = tf.global_variables_initializer()\n",
    "            with tf.Session() as sess:\n",
    "                sess.run(init)\n",
    "                enc_outputs,enc_laststate=sess.run([self.encoder_outputs,self.encoder_last_state], \n",
    "                                                   feed_dict={self.encoder_embedding_placeholder:emb ,\n",
    "                                                              self.encoder_inputs:eInput, \n",
    "                                                              self.encoder_inputs_length: eLengths })\n",
    "                print('encoder Outputs:',enc_outputs.shape)\n",
    "                print(enc_outputs)\n",
    "                print()\n",
    "                print('Encoder last state:',len(enc_laststate))\n",
    "                print(enc_laststate)\n",
    "            '''\n",
    "        \n",
    "\n",
    "        def build_decoder(self):\n",
    "            print('Building decoder and attention...')\n",
    "            with tf.variable_scope('decoder'):\n",
    "                \n",
    "                #Recheck this code\n",
    "                self.decoder_cell,self.decoder_initial_state = self.build_decoder_cell()\n",
    "                \n",
    "                #Instantiating pretrained embeddings\n",
    "                embedding_variable = tf.Variable(tf.constant(0.0, shape = [self.decoder_vocab_size, embedding_size]),trainable = False, name = 'embedding')\n",
    "\n",
    "                self.decoder_embedding_placeholder = tf.placeholder(tf.float32, shape=[self.decoder_vocab_size,embedding_size], name = 'embedding_placeholder' )\n",
    "                self.decoder_embeddings = embedding_variable.assign(self.decoder_embedding_placeholder)\n",
    "                self.decoder_inputs_embedded=tf.nn.embedding_lookup(self.decoder_embeddings,self.encoder_inputs)\n",
    "                \n",
    "                #instantiating dense layer --> DOUBT\n",
    "                input_layer = Dense(self.hidden_units, dtype = self.dtype, name = 'input_projection')\n",
    "                \n",
    "                #Output projection layer to convert cell outputs to logits --> DOUBT\n",
    "                output_layer = Dense(vocab_size,\"output_projection\")\n",
    "                \n",
    "                if self.mode == 'train':\n",
    "                    #decoder_inputs_embedded: [batch_size,max_time_step,embedding_size]\n",
    "                    self.decoder_inputs_embededded = tf.nn.embedding_lookup(self.decoder_embeddings,\n",
    "                                                                           self.decoder_inputs_train)\n",
    "                    \n",
    "                    #Embedded inputs going through projection layer\n",
    "                    self.decoder_inputs_embedded=input_layer(self.decoder_inputs_embedded)\n",
    "                    \n",
    "                    #Helper to feed inputs for training: read inputs from dense ground truth vectors\n",
    "                    training_helper = seq2seq.TrainingHelper(inputs = self.decoder_inputs_embedded,\n",
    "                                                            sequence_length=self.decoder_inputs_length,\n",
    "                                                            time_major=False,\n",
    "                                                            name='training_helper')\n",
    "                    training_decoder = seq2seq.BasicDecoder(cell=self.decoder_cell,\n",
    "                                                           helper = training_helper,\n",
    "                                                           initial_state = self.decoder_initial_state,\n",
    "                                                           output_layer = outputl_layer)\n",
    "                    \n",
    "                    #Maximum decoder time_steps in current batch\n",
    "                    max_decoder_length = tf.reduce_max(self.decoder_inputs_length)\n",
    "                    \n",
    "                    # decoder_outputs_train: BasicDecoderOutput\n",
    "                    #                        namedtuple(rnn_outputs, sample_id)\n",
    "                    # decoder_outputs_train.rnn_output: [batch_size, max_time_step + 1, num_decoder_symbols] if output_time_major=False\n",
    "                    #                                   [max_time_step + 1, batch_size, num_decoder_symbols] if output_time_major=True\n",
    "                    # decoder_outputs_train.sample_id: [batch_size], tf.int32\n",
    "                    (self.decoder_outputs_train, self.decoder_last_state_train, \n",
    "                    self.decoder_outputs_length_train) = (seq2seq.dynamic_decode(\n",
    "                    decoder=training_decoder,\n",
    "                    output_time_major=False,\n",
    "                    impute_finished=True,\n",
    "                    maximum_iterations=max_decoder_length))\n",
    "                    \n",
    "                    \n",
    "                    # More efficient to do the projection on the batch-time-concatenated tensor\n",
    "                    # logits_train: [batch_size, max_time_step + 1, num_decoder_symbols]\n",
    "                    # self.decoder_logits_train = output_layer(self.decoder_outputs_train.rnn_output)\n",
    "                    self.decoder_logits_train = tf.identity(self.decoder_outputs_train.rnn_output) \n",
    "                    # Use argmax to extract decoder symbols to emit\n",
    "                    self.decoder_pred_train = tf.argmax(self.decoder_logits_train, axis=-1,\n",
    "                                                        name='decoder_pred_train')\n",
    "                    \n",
    "                    # masks: masking for valid and padded time steps, [batch_size, max_time_step + 1]\n",
    "                    masks = tf.sequence_mask(lengths=self.decoder_inputs_length, \n",
    "                                         maxlen=max_decoder_length, dtype=self.dtype, name='masks')\n",
    "\n",
    "                    # Computes per word average cross-entropy over a batch\n",
    "                    # Internally calls 'nn_ops.sparse_softmax_cross_entropy_with_logits' by default\n",
    "                    self.loss = seq2seq.sequence_loss(logits=self.decoder_logits_train, \n",
    "                                                  targets=self.decoder_targets,\n",
    "                                                  weights=masks,\n",
    "                                                  average_across_timesteps=True,\n",
    "                                                  average_across_batch=True,)\n",
    "                    # Training summary for the current batch_loss\n",
    "                    \n",
    "                    # Training summary for the current batch_loss\n",
    "                    tf.summary.scalar('loss', self.loss)\n",
    "\n",
    "                    # Contruct graphs for minimizing loss\n",
    "                    self.init_optimizer()\n",
    "                 \n",
    "                \n",
    "                #NOT TRAINING\n",
    "                elif self.mode == 'decode':\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model..\n",
      "Building Encoder..\n",
      "encoder Outputs: (6, 20, 150)\n",
      "[[[-0.05762042  0.02685566  0.00973333 ... -0.10139786 -0.08643378\n",
      "   -0.00411691]\n",
      "  [-0.11273073 -0.04082831  0.01372339 ... -0.18441643 -0.14985572\n",
      "    0.06799476]\n",
      "  [-0.15240778 -0.03633224  0.07021248 ... -0.19439423 -0.18385549\n",
      "    0.14006957]\n",
      "  ...\n",
      "  [-0.1984036   0.13669704  0.22605549 ... -0.17484288 -0.03526106\n",
      "    0.09680352]\n",
      "  [-0.0511231   0.03034962  0.18782715 ... -0.19914497  0.08092974\n",
      "   -0.01022786]\n",
      "  [-0.03546507  0.00767517  0.13554008 ... -0.16243319  0.08822359\n",
      "    0.01649233]]\n",
      "\n",
      " [[-0.07305688  0.05720627  0.10539098 ... -0.05799546 -0.10593469\n",
      "   -0.00916362]\n",
      "  [-0.1250463   0.09961725  0.18959376 ... -0.10241318 -0.02890493\n",
      "    0.02321122]\n",
      "  [-0.01632207  0.03634805  0.0875487  ... -0.07028568 -0.00133933\n",
      "    0.00139211]\n",
      "  ...\n",
      "  [-0.19145322  0.1008764  -0.11696041 ... -0.22059815 -0.01195039\n",
      "    0.12622695]\n",
      "  [-0.19516967  0.0559051  -0.07420645 ... -0.2459103  -0.00667882\n",
      "    0.11362134]\n",
      "  [-0.16108772  0.03132381 -0.08791737 ... -0.19502582  0.03678241\n",
      "    0.08989111]]\n",
      "\n",
      " [[-0.09980132  0.10923197  0.07554918 ... -0.08616614 -0.14356467\n",
      "   -0.02105739]\n",
      "  [-0.15242769  0.07483725  0.0872784  ... -0.18811233 -0.1378575\n",
      "   -0.04629179]\n",
      "  [-0.14788663  0.02498998  0.07053103 ... -0.23580398 -0.19089657\n",
      "    0.01126196]\n",
      "  ...\n",
      "  [-0.20434414  0.07258259  0.05144683 ... -0.2473299  -0.0853696\n",
      "    0.16630638]\n",
      "  [-0.17733982  0.09845383  0.11751468 ... -0.18970568 -0.13321829\n",
      "    0.13023694]\n",
      "  [-0.13735145  0.04275718  0.09867662 ... -0.20533861 -0.05817715\n",
      "    0.09887679]]\n",
      "\n",
      " [[-0.09980132  0.10923197  0.07554918 ... -0.08616614 -0.14356467\n",
      "   -0.02105739]\n",
      "  [-0.15242769  0.07483725  0.0872784  ... -0.18811233 -0.1378575\n",
      "   -0.04629179]\n",
      "  [-0.14788663  0.02498998  0.07053103 ... -0.23580398 -0.19089657\n",
      "    0.01126196]\n",
      "  ...\n",
      "  [-0.1725343  -0.03444435  0.03963805 ... -0.2703142  -0.10285386\n",
      "    0.19149718]\n",
      "  [-0.0825471  -0.01491215  0.01783722 ... -0.24046326 -0.17791012\n",
      "    0.25130522]\n",
      "  [-0.09341691 -0.02204679 -0.01336667 ... -0.20547193 -0.08278343\n",
      "    0.18454057]]\n",
      "\n",
      " [[-0.05762042  0.02685566  0.00973333 ... -0.10139786 -0.08643378\n",
      "   -0.00411691]\n",
      "  [-0.11273073 -0.04082831  0.01372339 ... -0.18441643 -0.14985572\n",
      "    0.06799476]\n",
      "  [-0.15240778 -0.03633224  0.07021248 ... -0.19439423 -0.18385549\n",
      "    0.14006957]\n",
      "  ...\n",
      "  [-0.11906931 -0.02837208  0.13864453 ... -0.27149376  0.12759206\n",
      "    0.20102811]\n",
      "  [-0.1247417  -0.03597463  0.21432124 ... -0.2126466   0.1159843\n",
      "    0.23883855]\n",
      "  [-0.11686427 -0.04965051  0.10302008 ... -0.17025821  0.08819171\n",
      "    0.19377837]]\n",
      "\n",
      " [[-0.09578019  0.01643262  0.11836822 ... -0.09388398  0.00526328\n",
      "    0.08147604]\n",
      "  [-0.14679727  0.10589882  0.14054063 ... -0.12334925 -0.13388693\n",
      "    0.04325385]\n",
      "  [-0.1342442   0.03773225  0.22853193 ... -0.20484145 -0.18078864\n",
      "    0.01278723]\n",
      "  ...\n",
      "  [-0.15833381  0.12820147  0.02311179 ... -0.19826877 -0.14183918\n",
      "   -0.02256591]\n",
      "  [-0.15895253  0.10823867  0.05452708 ... -0.22133724 -0.18398438\n",
      "   -0.03103005]\n",
      "  [-0.1632296   0.08173542  0.00935752 ... -0.19376224 -0.05261904\n",
      "   -0.01412435]]]\n",
      "\n",
      "Encoder last state: 1\n",
      "(LSTMStateTuple(c=array([[-7.28809685e-02,  1.65182240e-02,  2.83166289e-01,\n",
      "        -4.09194455e-02,  1.49797767e-01, -4.40237582e-01,\n",
      "         2.45563209e-01,  1.61001250e-01,  8.88524503e-02,\n",
      "         1.27842188e-01,  1.21982336e-01, -4.97131199e-02,\n",
      "         7.12123737e-02,  5.00251129e-02, -2.14143142e-01,\n",
      "        -6.88565075e-02,  7.90088326e-02,  9.83452573e-02,\n",
      "         1.73618682e-02, -3.51863950e-02,  1.58098221e-01,\n",
      "        -3.55206728e-01,  3.85211706e-01, -8.45327228e-02,\n",
      "        -1.86897278e-01,  2.13520467e-01,  4.25116956e-01,\n",
      "         1.79221362e-01,  1.05069824e-01,  1.07631676e-01,\n",
      "        -5.11683226e-01, -3.66663218e-01, -3.26951712e-01,\n",
      "         1.92909643e-01, -2.33380824e-01,  1.66535333e-01,\n",
      "         1.03358299e-01, -3.84742320e-01, -1.22163758e-01,\n",
      "        -1.93866566e-01, -2.23157346e-01,  1.84808314e-01,\n",
      "        -3.93609226e-01, -4.43062723e-01,  1.74462929e-01,\n",
      "         9.23104119e-03,  5.60237058e-02,  9.22387093e-02,\n",
      "        -9.97272372e-01,  1.55591249e-01, -3.37660611e-01,\n",
      "        -3.87016207e-01,  8.22393969e-03,  1.08747836e-03,\n",
      "         9.65730622e-02, -1.98615808e-02, -5.46425059e-02,\n",
      "         4.20340627e-01,  1.28007948e-01,  1.15207888e-01,\n",
      "        -2.76798189e-01, -2.34023094e-01,  5.68400383e-01,\n",
      "         2.63509780e-01,  1.58959091e-01, -4.69957739e-01,\n",
      "         7.52467066e-02,  3.49758044e-02, -4.93370295e-02,\n",
      "        -1.93448067e-01,  1.18691027e-01,  6.39514923e-02,\n",
      "        -9.73467305e-02, -2.45333999e-01,  3.53623062e-01,\n",
      "         1.83575094e-01,  5.33486009e-01,  2.52428383e-01,\n",
      "        -1.48139462e-01, -1.72714628e-02,  7.59528577e-02,\n",
      "         2.14529544e-01, -1.85856834e-01, -2.52973318e-01,\n",
      "         3.27126175e-01,  2.34468207e-02,  1.65713914e-02,\n",
      "         9.28192437e-02, -7.27098435e-02,  5.07631660e-01,\n",
      "         2.92350829e-01,  2.08555594e-01,  2.09544748e-01,\n",
      "        -2.58517921e-01, -2.00967506e-01, -1.71351135e-01,\n",
      "        -4.32720780e-01, -3.49452794e-01, -3.40823889e-01,\n",
      "         9.61543918e-02, -5.48825972e-02,  2.37676904e-01,\n",
      "        -7.46793523e-02, -9.47537273e-02,  9.27502662e-03,\n",
      "         2.20175758e-01,  3.45821887e-01,  1.33392707e-01,\n",
      "         5.42229339e-02,  1.98025063e-01,  1.84598610e-01,\n",
      "         4.56016734e-02, -7.41604984e-01, -3.81790474e-02,\n",
      "        -1.70664728e-01, -4.45823297e-02,  2.09973693e-01,\n",
      "        -1.50824904e-01,  1.24900423e-01, -1.08614564e-03,\n",
      "        -4.58536863e-01, -7.07643390e-01,  2.79553890e-01,\n",
      "         3.60703826e-01,  4.16118681e-01, -1.00499436e-01,\n",
      "         3.78809094e-01, -9.15126726e-02,  3.75974886e-02,\n",
      "         3.45072180e-01,  2.25420833e-01,  2.62531936e-01,\n",
      "        -2.42561206e-01,  2.06398427e-01,  1.61093608e-01,\n",
      "         4.66265202e-01, -1.00066826e-01,  4.86950159e-01,\n",
      "         1.55659646e-01, -1.97006106e-01,  5.01661777e-01,\n",
      "         6.88593566e-01, -3.07749182e-01,  3.07346821e-01,\n",
      "         1.71346888e-02, -1.24740660e-01,  1.08539902e-01,\n",
      "        -3.46905828e-01,  1.71578810e-01,  3.41091082e-02],\n",
      "       [-3.40037614e-01,  7.12568909e-02, -1.76172554e-01,\n",
      "         2.60523371e-02,  1.69430077e-01, -1.34932429e-01,\n",
      "         7.20928848e-01,  1.57974020e-01, -3.64338219e-01,\n",
      "         7.32341856e-02,  7.35950917e-02,  3.53714600e-02,\n",
      "         1.67799577e-01,  1.79832429e-01, -1.52040973e-01,\n",
      "        -1.24329269e-01, -2.25379825e-01, -4.00594652e-01,\n",
      "         1.27465576e-01, -3.70870680e-01,  2.16022849e-01,\n",
      "        -4.99897659e-01,  2.79965460e-01, -2.46500418e-01,\n",
      "        -8.76104087e-02,  3.06318730e-01,  3.30701843e-02,\n",
      "        -6.20269328e-02, -1.18227705e-01,  5.49645424e-02,\n",
      "        -4.07391101e-01, -2.82956004e-01, -3.73207122e-01,\n",
      "         1.02739073e-02, -2.15194687e-01,  1.78499728e-01,\n",
      "         2.12761045e-01, -5.14189243e-01, -4.38194066e-01,\n",
      "        -3.43850017e-01, -4.84390646e-01,  6.07095063e-01,\n",
      "        -3.86619449e-01, -2.51469374e-01, -4.43884805e-02,\n",
      "        -1.84337385e-02,  1.57645807e-01, -2.63240457e-01,\n",
      "        -4.14230108e-01,  3.21586609e-01, -2.73446679e-01,\n",
      "        -4.07236725e-01, -6.75786659e-03,  3.93322855e-01,\n",
      "         2.25041807e-01, -2.30655000e-02, -6.75105751e-02,\n",
      "         4.45359796e-01,  3.86803076e-02, -1.11270562e-01,\n",
      "        -1.37975067e-01, -9.19562429e-02,  2.13788524e-01,\n",
      "        -3.31681222e-02, -4.66083586e-02, -3.10412571e-02,\n",
      "        -7.88389266e-01,  4.33105305e-02, -3.85443449e-01,\n",
      "        -6.38537854e-02, -2.17617288e-01, -3.17293048e-01,\n",
      "        -2.41081715e-01,  1.28184676e-01, -5.82375526e-02,\n",
      "         1.14475854e-01,  3.88175905e-01,  1.67827308e-01,\n",
      "        -1.15813941e-01,  2.61912271e-02,  2.60456532e-01,\n",
      "        -1.23751819e-01, -1.06144682e-01,  3.64194185e-01,\n",
      "         2.05069453e-01,  1.11881897e-01,  4.01937366e-01,\n",
      "         1.53830543e-01, -4.94159818e-01,  6.43370807e-01,\n",
      "         2.43237793e-01,  1.87521368e-01,  5.54536223e-01,\n",
      "         6.11580536e-02, -7.63118491e-02, -5.40671051e-01,\n",
      "        -3.88050973e-02, -3.41688544e-01, -7.64463991e-02,\n",
      "         3.96791220e-01, -1.81681961e-01, -2.91668862e-01,\n",
      "        -7.01552927e-02, -2.67546177e-02, -5.38713112e-02,\n",
      "         2.92982042e-01,  1.65568978e-01,  3.99923809e-02,\n",
      "        -9.88886878e-02,  1.87703565e-01,  2.43328735e-01,\n",
      "        -1.54683977e-01, -5.38057268e-01,  3.02936882e-01,\n",
      "         1.27856076e-01, -6.21894225e-02, -2.63876244e-02,\n",
      "         6.96839839e-02, -4.23277766e-02,  4.04908359e-01,\n",
      "        -2.06282392e-01, -2.73313820e-01,  7.40594864e-02,\n",
      "        -8.02689642e-02,  2.34111756e-01,  1.85371935e-01,\n",
      "         2.78390199e-01, -5.81267923e-02, -4.26101536e-02,\n",
      "         1.22193098e-01, -1.02578122e-02,  3.87685448e-01,\n",
      "        -1.54595673e-01,  3.49969685e-01,  4.11354989e-01,\n",
      "         3.18010807e-01, -1.02157846e-01,  3.13361846e-02,\n",
      "         4.01131921e-02, -4.20012847e-02,  2.05186337e-01,\n",
      "         2.55722404e-01,  1.55309252e-02,  1.33654982e-01,\n",
      "         1.82541739e-02,  2.41634116e-01, -1.75096571e-01,\n",
      "        -4.04360145e-01,  7.16633573e-02,  1.77298784e-01],\n",
      "       [-2.88280457e-01,  9.75636542e-02,  2.11968482e-01,\n",
      "        -2.83671282e-02,  4.27787043e-02, -4.59224075e-01,\n",
      "         3.17300200e-01,  1.90518349e-02, -2.25053597e-02,\n",
      "         1.15822300e-01,  2.67444760e-01,  5.16837239e-02,\n",
      "         1.83218077e-01, -2.47678645e-02, -5.77614233e-02,\n",
      "        -1.86267391e-01,  3.07402182e-02, -1.57247111e-01,\n",
      "         1.52354762e-01, -2.25222260e-02,  4.10892278e-01,\n",
      "        -5.67960620e-01,  2.83429891e-01, -3.27838302e-01,\n",
      "        -7.12406635e-03,  8.04694146e-02,  2.01499641e-01,\n",
      "        -1.15321271e-01,  6.63104355e-02,  1.22033343e-01,\n",
      "        -5.32937884e-01, -2.93478847e-01, -5.21937549e-01,\n",
      "         3.11018471e-02, -4.37066674e-01,  2.52281278e-01,\n",
      "         1.45562738e-01, -4.92345870e-01, -4.69632685e-01,\n",
      "        -2.05621168e-01, -3.08868229e-01,  1.86571375e-01,\n",
      "        -3.16788226e-01, -3.82492572e-01,  2.33670637e-01,\n",
      "         1.25235677e-01, -2.72652149e-01,  1.54195800e-01,\n",
      "        -9.34104204e-01,  3.51507843e-01, -5.82098067e-01,\n",
      "        -7.38226652e-01, -2.19430014e-01,  2.94947684e-01,\n",
      "         9.47758481e-02,  1.07702948e-01, -7.57437125e-02,\n",
      "         4.30752248e-01,  1.88569665e-01,  1.22898862e-01,\n",
      "        -2.97080219e-01, -3.55236232e-01,  3.48706186e-01,\n",
      "         1.35380164e-01, -5.63623160e-02, -5.22330463e-01,\n",
      "        -6.23795241e-02, -6.90578073e-02, -1.27052993e-01,\n",
      "         1.77407414e-01, -5.78750484e-02,  3.56438011e-02,\n",
      "        -4.68301261e-03, -2.76018202e-01,  2.44196922e-01,\n",
      "         3.49158227e-01,  3.24453861e-01,  1.84576988e-01,\n",
      "        -2.50314206e-01,  5.27123064e-02,  2.87077039e-01,\n",
      "         1.62163839e-01, -9.37636942e-04,  1.39807150e-01,\n",
      "         5.87235987e-01,  5.59988260e-01,  2.35890746e-01,\n",
      "        -2.04577923e-01, -1.77837923e-01,  3.40730965e-01,\n",
      "         2.63761520e-01,  7.69954920e-02,  1.83066249e-01,\n",
      "         2.62800828e-02, -1.72717180e-02, -5.79236805e-01,\n",
      "        -3.17591250e-01, -4.52969193e-01, -3.82047802e-01,\n",
      "         1.42526865e-01, -2.58621961e-01,  2.74456143e-01,\n",
      "        -2.22892836e-01, -1.21339560e-01, -1.14509523e-01,\n",
      "         4.16276693e-01,  3.88830811e-01,  2.36096248e-01,\n",
      "         1.20431870e-01,  2.86022186e-01,  1.34672225e-01,\n",
      "         2.10699812e-03, -4.45410550e-01,  2.61965454e-01,\n",
      "         4.93301153e-02, -2.56698877e-01,  4.99077648e-01,\n",
      "        -6.09561689e-02,  2.15996265e-01,  1.98559053e-02,\n",
      "        -4.99911070e-01, -5.06314814e-01,  2.41341472e-01,\n",
      "         8.79868194e-02,  3.29474777e-01,  1.47080392e-01,\n",
      "         4.93465692e-01, -1.85424849e-01, -1.96529329e-01,\n",
      "         2.65405893e-01,  1.06568389e-01,  4.97021407e-01,\n",
      "        -2.81408310e-01,  2.87922204e-01,  4.84786302e-01,\n",
      "         4.22885150e-01, -2.05363154e-01,  3.31291854e-01,\n",
      "         1.97676182e-01, -2.28963673e-01,  4.42553967e-01,\n",
      "         7.03691363e-01, -2.34509557e-01,  3.01744342e-01,\n",
      "        -1.41424641e-01,  5.07479161e-02,  1.66900888e-01,\n",
      "        -4.31164116e-01, -1.12734117e-01,  1.90077588e-01],\n",
      "       [-2.00126410e-01, -4.94335070e-02, -2.70529110e-02,\n",
      "        -1.07542112e-01, -7.43145570e-02, -4.94045258e-01,\n",
      "         7.36477375e-01,  1.94610611e-01, -2.92838383e-02,\n",
      "         2.21132666e-01,  2.79537141e-01, -6.28323555e-02,\n",
      "        -2.28161365e-02,  3.23929578e-01, -1.74542651e-01,\n",
      "        -1.98394328e-01, -3.62381749e-02, -3.34403012e-03,\n",
      "         3.24265271e-01, -4.99539733e-01,  1.08122468e-01,\n",
      "        -7.24064708e-01,  2.43735403e-01, -1.55556813e-01,\n",
      "        -4.26738620e-01,  2.87104785e-01,  3.47582519e-01,\n",
      "        -9.50435251e-02, -1.85350895e-01,  3.41473371e-02,\n",
      "        -5.14800668e-01, -2.03257129e-01, -6.09958589e-01,\n",
      "         2.45599523e-01, -3.42678308e-01,  3.82584214e-01,\n",
      "         7.77549297e-02, -7.77616620e-01, -2.96158046e-01,\n",
      "        -5.14766872e-01, -4.02380466e-01,  5.28254390e-01,\n",
      "        -2.55454183e-01, -3.71381283e-01,  2.14417279e-01,\n",
      "         6.60817474e-02,  3.30773592e-02, -1.77208245e-01,\n",
      "        -7.68858552e-01,  1.92623362e-02, -4.78669286e-01,\n",
      "        -5.12296677e-01,  1.86669245e-01,  3.21492970e-01,\n",
      "         3.05273652e-01,  1.14693530e-02, -1.41827911e-01,\n",
      "         3.22549820e-01, -2.31253169e-02,  2.24451683e-02,\n",
      "        -1.78222120e-01, -1.83276385e-01,  3.91975194e-01,\n",
      "         7.78222457e-02, -1.77123114e-01, -4.33466405e-01,\n",
      "        -8.84230852e-01,  3.40294540e-02, -8.29163417e-02,\n",
      "        -4.49746028e-02, -4.23685223e-01, -7.21083283e-02,\n",
      "        -2.87986636e-01, -1.65792391e-01, -3.76287252e-01,\n",
      "         1.00845814e-01,  5.36966741e-01,  1.24179155e-01,\n",
      "        -1.82338431e-01, -2.45528683e-01,  4.47618842e-01,\n",
      "         2.26033628e-01, -3.52316380e-01,  1.92107737e-01,\n",
      "         2.21775383e-01,  9.37661454e-02,  1.12873740e-01,\n",
      "         2.45899439e-01, -9.84252915e-02,  6.02517068e-01,\n",
      "         2.95619607e-01,  2.96042740e-01,  5.90913773e-01,\n",
      "        -2.44180635e-01, -5.66293001e-02, -5.63442945e-01,\n",
      "        -1.08985864e-02, -3.86986256e-01, -1.44655496e-01,\n",
      "         3.86945724e-01, -3.08692664e-01, -2.61883140e-01,\n",
      "         1.36644384e-02, -9.42016616e-02, -4.25135717e-02,\n",
      "         4.05054778e-01,  6.00101426e-02,  4.01657335e-02,\n",
      "        -1.49690837e-01,  1.14018165e-01,  1.88241526e-01,\n",
      "        -7.75799900e-02, -4.41818297e-01,  3.70180130e-01,\n",
      "         1.08617060e-02, -3.66897076e-01, -1.25378340e-01,\n",
      "        -1.35936305e-01,  1.87231954e-02,  6.16735339e-01,\n",
      "        -1.91201881e-01, -7.16193259e-01, -9.37272981e-02,\n",
      "        -2.15415478e-01,  4.63964701e-01,  1.80088505e-01,\n",
      "         1.74622566e-01, -2.64035076e-01,  2.12459832e-01,\n",
      "         6.82673007e-02,  4.15654406e-02,  4.75579649e-01,\n",
      "        -1.68664664e-01,  6.01432770e-02,  4.02835011e-01,\n",
      "         4.51072752e-01, -1.17990695e-01,  3.85706335e-01,\n",
      "        -1.91722691e-01, -3.61865997e-01,  3.56239349e-01,\n",
      "         1.78242028e-01, -2.23880395e-01, -7.92051628e-02,\n",
      "        -8.65261853e-02,  2.24173978e-01,  4.94445525e-02,\n",
      "        -4.31767195e-01, -1.62957728e-01,  3.73648286e-01],\n",
      "       [-2.38192067e-01, -1.12251461e-01,  2.09932566e-01,\n",
      "        -1.42212525e-01,  1.34835392e-03, -3.91875952e-01,\n",
      "         4.65635896e-01, -1.94045573e-01, -4.87727411e-02,\n",
      "        -2.06358880e-02,  2.92562932e-01, -4.79440950e-02,\n",
      "        -2.56019123e-02,  3.71410340e-01, -4.28250760e-01,\n",
      "        -1.24690682e-01, -6.46518394e-02, -1.39869034e-01,\n",
      "         2.54831791e-01, -2.65017182e-01,  2.14352727e-01,\n",
      "        -5.37625492e-01,  2.60090619e-01, -1.56287834e-01,\n",
      "        -2.43293852e-01,  1.10628799e-01,  2.39129916e-01,\n",
      "         2.25967795e-01, -7.42611215e-02,  1.62410259e-01,\n",
      "        -1.62366420e-01, -1.55707106e-01, -5.55827498e-01,\n",
      "         1.48512229e-01, -1.61744073e-01,  2.94661492e-01,\n",
      "        -3.51345725e-02, -4.63834167e-01, -1.57753050e-01,\n",
      "        -2.06171498e-01, -3.98526967e-01,  5.26810884e-02,\n",
      "        -5.71144342e-01, -4.98862326e-01,  1.03258274e-01,\n",
      "         1.47711672e-02,  6.09928556e-02, -1.71248332e-01,\n",
      "        -6.86689496e-01,  3.26058030e-01, -5.18642008e-01,\n",
      "        -4.59902197e-01, -4.56589535e-02,  1.35513544e-01,\n",
      "         1.60456166e-01, -1.46420643e-01, -1.08306020e-01,\n",
      "         1.39071360e-01,  1.10924609e-01,  1.11431703e-01,\n",
      "        -2.45111719e-01, -1.33896872e-01,  4.22118008e-01,\n",
      "        -1.15166128e-01, -9.75498408e-02, -2.80406773e-01,\n",
      "        -3.47458124e-01,  1.29265636e-01, -1.70769453e-01,\n",
      "        -3.58454399e-02,  3.18264157e-01, -1.92425996e-01,\n",
      "        -5.60439005e-02, -4.17595394e-02, -4.41517383e-02,\n",
      "         1.43394768e-01,  3.45660686e-01,  1.65647417e-01,\n",
      "        -1.68064177e-01, -1.26245856e-01,  7.07349479e-02,\n",
      "         8.68751332e-02, -7.03947470e-02,  1.41353622e-01,\n",
      "         3.45665991e-01,  1.83627456e-01,  2.77067035e-01,\n",
      "         6.34093732e-02, -3.63929838e-01,  4.86633033e-01,\n",
      "         3.28432381e-01,  2.14633107e-01,  2.27961242e-01,\n",
      "        -3.21183085e-01, -8.34031105e-02, -4.01812524e-01,\n",
      "        -7.69209564e-02, -3.95044148e-01, -2.59849075e-02,\n",
      "         3.74907434e-01, -3.08704615e-01, -3.59497592e-03,\n",
      "        -4.73391935e-02, -5.66534735e-02, -1.46205693e-01,\n",
      "         4.39799637e-01,  2.50643313e-01,  2.09599793e-01,\n",
      "         9.53493267e-02,  2.58398622e-01, -3.83278690e-02,\n",
      "        -1.44732399e-02, -5.19865632e-01,  2.70829439e-01,\n",
      "         3.70720513e-02, -2.39388317e-01, -4.85835671e-02,\n",
      "        -1.09922789e-01,  1.20869368e-01,  2.49234393e-01,\n",
      "        -1.72592282e-01, -5.09273529e-01,  1.64019138e-01,\n",
      "         2.16061085e-01,  4.84275103e-01, -3.14848125e-02,\n",
      "         3.84583890e-01,  5.43274358e-02,  7.12624490e-02,\n",
      "         1.41041517e-01, -2.14533061e-02,  2.12204248e-01,\n",
      "        -3.73566061e-01,  2.68181980e-01,  3.18883240e-01,\n",
      "         4.45057511e-01, -1.61023110e-01,  3.21009666e-01,\n",
      "        -5.88649549e-02, -4.48749065e-02,  6.06280416e-02,\n",
      "         4.38127548e-01, -1.41273543e-01,  1.81613445e-01,\n",
      "        -7.89813250e-02,  7.63909221e-02, -1.68736950e-01,\n",
      "        -3.69548380e-01,  1.74867868e-01,  3.96260202e-01],\n",
      "       [-3.45500946e-01,  1.85160667e-01,  1.86127685e-02,\n",
      "        -1.43529698e-01, -9.18369293e-02, -3.11761945e-01,\n",
      "         5.25884092e-01,  8.48189369e-03, -1.27477631e-01,\n",
      "         1.54467806e-01,  3.03012073e-01, -4.96460199e-02,\n",
      "         2.12290399e-02,  1.29562229e-01, -2.63374448e-01,\n",
      "        -6.33169115e-02, -9.72288027e-02, -3.90985191e-01,\n",
      "         2.80757546e-01, -3.65868896e-01,  3.29451621e-01,\n",
      "        -4.80770111e-01,  3.23103040e-01, -2.87627041e-01,\n",
      "        -1.25773996e-01,  1.86366826e-01,  6.24364555e-01,\n",
      "         8.81460384e-02, -7.97404274e-02,  7.22381398e-02,\n",
      "        -4.18095261e-01, -1.26961544e-01, -7.54181564e-01,\n",
      "        -5.92215806e-02, -2.58827329e-01,  4.74800050e-01,\n",
      "         4.24537539e-01, -6.89407766e-01, -1.74599126e-01,\n",
      "        -2.99264133e-01, -4.25855279e-01,  3.34091187e-01,\n",
      "        -4.87690240e-01, -4.69156593e-01,  5.69576509e-02,\n",
      "         2.81034075e-02, -2.22881421e-01,  1.32539511e-01,\n",
      "        -8.58590841e-01,  4.01093870e-01, -2.09366620e-01,\n",
      "        -2.20218256e-01, -2.14014024e-01,  5.39255515e-02,\n",
      "         2.78838366e-01, -1.89572603e-01, -5.91743253e-02,\n",
      "         4.63643968e-01, -1.16548762e-02,  7.07297474e-02,\n",
      "        -4.85736370e-01, -1.33564249e-01,  6.04725480e-01,\n",
      "         4.79248837e-02, -1.69686794e-01, -7.35915899e-01,\n",
      "        -5.65937757e-01, -4.23946410e-01, -3.43484163e-01,\n",
      "         1.66488085e-02, -1.84464231e-01, -3.26599300e-01,\n",
      "        -1.34442225e-01, -1.19551502e-01, -6.00547381e-02,\n",
      "        -7.20831081e-02,  6.32363617e-01,  3.63511831e-01,\n",
      "        -1.08287893e-01,  7.16556907e-02,  2.10792273e-02,\n",
      "         1.06001228e-01, -3.12411543e-02,  2.90100694e-01,\n",
      "         1.55844986e-01,  2.24278584e-01,  3.69902104e-02,\n",
      "         3.12966928e-02, -3.59562397e-01,  8.36338699e-01,\n",
      "         2.04294980e-01,  1.50808811e-01,  3.48950446e-01,\n",
      "        -9.77308303e-02, -1.82491660e-01, -5.34097254e-01,\n",
      "        -2.70123929e-01, -5.52695692e-01, -1.37496665e-02,\n",
      "         5.12964189e-01, -5.58022633e-02,  1.13822609e-01,\n",
      "        -1.10288583e-01, -4.44143265e-01,  2.06693113e-01,\n",
      "         3.93935025e-01,  1.58339024e-01,  1.74104840e-01,\n",
      "         2.14752063e-01,  2.35718578e-01,  2.72106737e-01,\n",
      "         1.44621700e-01, -4.79502678e-01,  3.95246565e-01,\n",
      "        -1.32853284e-01, -2.36186713e-01,  2.43424177e-02,\n",
      "        -3.17935288e-01,  2.99298167e-01,  3.08011204e-01,\n",
      "        -3.46017748e-01, -5.61242878e-01,  1.05065346e-01,\n",
      "        -7.20241219e-02,  1.57456443e-01,  1.17429867e-01,\n",
      "         1.59029722e-01, -1.30719781e-01,  2.74811745e-01,\n",
      "         1.73236117e-01,  1.10815190e-01,  8.28424543e-02,\n",
      "         1.87356845e-01,  5.54770589e-01,  3.67641151e-01,\n",
      "         6.51642621e-01, -1.98961169e-01,  3.17653507e-01,\n",
      "         1.71557784e-01, -4.55606073e-01,  6.37572348e-01,\n",
      "         5.75596213e-01, -2.85982996e-01,  3.69216144e-01,\n",
      "         2.55252626e-02, -1.01195469e-01, -2.05207974e-01,\n",
      "        -3.82106036e-01, -9.90478694e-02, -2.69964654e-02]], dtype=float32), h=array([[-0.03546507,  0.00767517,  0.13554008, -0.01932578,  0.0775651 ,\n",
      "        -0.21823101,  0.11773086,  0.08331852,  0.0456522 ,  0.06208936,\n",
      "         0.06267916, -0.02418587,  0.03396619,  0.02459249, -0.1093533 ,\n",
      "        -0.03381573,  0.0378    ,  0.04876009,  0.00834868, -0.01833895,\n",
      "         0.07296706, -0.17775734,  0.19194904, -0.04210888, -0.08947743,\n",
      "         0.1130982 ,  0.19705822,  0.08848125,  0.0528662 ,  0.05224811,\n",
      "        -0.24105795, -0.17672619, -0.16689515,  0.08817869, -0.1106116 ,\n",
      "         0.08711781,  0.05385622, -0.19505909, -0.062691  , -0.09284339,\n",
      "        -0.10782751,  0.0945811 , -0.19591455, -0.2050467 ,  0.08802812,\n",
      "         0.00438754,  0.02759898,  0.04820264, -0.40390798,  0.08227302,\n",
      "        -0.16244043, -0.18129434,  0.00385047,  0.00053463,  0.05093596,\n",
      "        -0.01041947, -0.0266778 ,  0.20512116,  0.06595121,  0.05838167,\n",
      "        -0.13292287, -0.11809303,  0.26868662,  0.12877147,  0.07672317,\n",
      "        -0.22567332,  0.03727192,  0.01654618, -0.02586597, -0.09526726,\n",
      "         0.05731681,  0.03043798, -0.05239054, -0.12134154,  0.1752064 ,\n",
      "         0.0910911 ,  0.24128014,  0.12534726, -0.08015046, -0.0085934 ,\n",
      "         0.03946853,  0.10809135, -0.0847827 , -0.1284187 ,  0.15023032,\n",
      "         0.01201562,  0.00793589,  0.04830987, -0.03778707,  0.23210493,\n",
      "         0.14414266,  0.10470155,  0.09999523, -0.11986329, -0.10225271,\n",
      "        -0.08831839, -0.19003904, -0.17574872, -0.16314046,  0.0492352 ,\n",
      "        -0.02606877,  0.11133087, -0.03603724, -0.04384073,  0.00487466,\n",
      "         0.10534713,  0.16049947,  0.0711182 ,  0.02786181,  0.09846155,\n",
      "         0.09786578,  0.02243279, -0.28922892, -0.02032108, -0.08474611,\n",
      "        -0.02176461,  0.11204319, -0.07564866,  0.06340782, -0.00054303,\n",
      "        -0.21750923, -0.32283193,  0.13721466,  0.16476505,  0.19317375,\n",
      "        -0.05260782,  0.18479255, -0.04986883,  0.01892237,  0.16532977,\n",
      "         0.10884504,  0.1285176 , -0.11899255,  0.10035734,  0.08090561,\n",
      "         0.20119233, -0.04873588,  0.20984115,  0.07878663, -0.10053852,\n",
      "         0.21147715,  0.2944806 , -0.14962648,  0.14605178,  0.00847406,\n",
      "        -0.0618998 ,  0.04988625, -0.16243319,  0.08822359,  0.01649233],\n",
      "       [-0.16108772,  0.03132381, -0.08791737,  0.0124654 ,  0.08956291,\n",
      "        -0.07347858,  0.29948986,  0.07797033, -0.1717    ,  0.03637671,\n",
      "         0.03759741,  0.01762506,  0.07687477,  0.09070171, -0.08087384,\n",
      "        -0.05920609, -0.11111485, -0.18090655,  0.06587114, -0.17966764,\n",
      "         0.10768215, -0.25435635,  0.14201745, -0.11704261, -0.04461046,\n",
      "         0.15741777,  0.016407  , -0.03155274, -0.05559203,  0.02571799,\n",
      "        -0.19122034, -0.14097336, -0.179867  ,  0.00527175, -0.09734909,\n",
      "         0.08909372,  0.10697893, -0.2476559 , -0.20710503, -0.16049087,\n",
      "        -0.223714  ,  0.28557172, -0.1975651 , -0.1217638 , -0.02194972,\n",
      "        -0.00861994,  0.07521407, -0.13475436, -0.20162225,  0.16095792,\n",
      "        -0.13435706, -0.19274403, -0.00328435,  0.18198982,  0.11686387,\n",
      "        -0.01211034, -0.034642  ,  0.21712708,  0.01861979, -0.05792824,\n",
      "        -0.06796011, -0.04479939,  0.09767763, -0.01647861, -0.023136  ,\n",
      "        -0.01585313, -0.31691405,  0.02144942, -0.18599431, -0.02953387,\n",
      "        -0.1109321 , -0.14052948, -0.12443092,  0.06296333, -0.03092438,\n",
      "         0.05738477,  0.19656478,  0.0892857 , -0.0608427 ,  0.01304901,\n",
      "         0.12676913, -0.06012761, -0.04710722,  0.17213336,  0.09873127,\n",
      "         0.05720169,  0.18074222,  0.07682636, -0.23488745,  0.2729557 ,\n",
      "         0.1212697 ,  0.0898132 ,  0.26455373,  0.02930335, -0.04131934,\n",
      "        -0.2486562 , -0.01832996, -0.16542375, -0.03860741,  0.18494518,\n",
      "        -0.09052255, -0.14525238, -0.03739135, -0.01299236, -0.0281609 ,\n",
      "         0.14097425,  0.08150238,  0.02082587, -0.04905379,  0.09152088,\n",
      "         0.13495274, -0.0698707 , -0.2380998 ,  0.15238307,  0.06280241,\n",
      "        -0.02911764, -0.01329803,  0.0328562 , -0.02117179,  0.19035208,\n",
      "        -0.10646386, -0.13245814,  0.03716189, -0.03939734,  0.12473614,\n",
      "         0.10076775,  0.12275299, -0.03101344, -0.02254413,  0.0586047 ,\n",
      "        -0.004925  ,  0.17858581, -0.07309564,  0.16483657,  0.20378481,\n",
      "         0.13813646, -0.04931963,  0.01490247,  0.01968529, -0.02128761,\n",
      "         0.09283995,  0.12461638,  0.00802924,  0.06651933,  0.0090577 ,\n",
      "         0.1236894 , -0.07829162, -0.19502582,  0.03678241,  0.08989111],\n",
      "       [-0.13735145,  0.04275718,  0.09867662, -0.01364343,  0.02172375,\n",
      "        -0.2395442 ,  0.14504941,  0.01009089, -0.01127355,  0.05749023,\n",
      "         0.13558516,  0.02408077,  0.08740666, -0.01245429, -0.03002829,\n",
      "        -0.08762942,  0.01465436, -0.07859185,  0.07535474, -0.01175259,\n",
      "         0.18248399, -0.2794545 ,  0.14223076, -0.16047819, -0.00358973,\n",
      "         0.04542708,  0.09724921, -0.05910112,  0.03218007,  0.0599944 ,\n",
      "        -0.25100237, -0.15038091, -0.23976043,  0.01487289, -0.19580519,\n",
      "         0.12698649,  0.07868312, -0.24536262, -0.21975656, -0.09810246,\n",
      "        -0.14403307,  0.0993099 , -0.1562171 , -0.18702313,  0.12040178,\n",
      "         0.05839607, -0.12615837,  0.08362245, -0.38039693,  0.1747263 ,\n",
      "        -0.27500892, -0.3106475 , -0.10239015,  0.13320503,  0.049683  ,\n",
      "         0.05802315, -0.04006302,  0.20133606,  0.09614523,  0.06180259,\n",
      "        -0.1397283 , -0.17308302,  0.16828078,  0.06710278, -0.02760465,\n",
      "        -0.24576183, -0.03011064, -0.03473804, -0.06759516,  0.0862354 ,\n",
      "        -0.02754468,  0.01676512, -0.0025059 , -0.1332479 ,  0.12770544,\n",
      "         0.17562674,  0.16224977,  0.09349193, -0.1338633 ,  0.02641259,\n",
      "         0.143839  ,  0.08124826, -0.00043615,  0.06884957,  0.24821939,\n",
      "         0.26605505,  0.10695365, -0.10247687, -0.09169989,  0.16591409,\n",
      "         0.132478  ,  0.03941121,  0.08864591,  0.01232341, -0.00937608,\n",
      "        -0.26542273, -0.14486219, -0.21171935, -0.18704706,  0.07437032,\n",
      "        -0.1200804 ,  0.13288255, -0.10291824, -0.05925158, -0.05911076,\n",
      "         0.19624873,  0.16774526,  0.11686078,  0.06009952,  0.13757864,\n",
      "         0.07369465,  0.00104769, -0.18950038,  0.13302939,  0.02539769,\n",
      "        -0.1219489 ,  0.24349011, -0.03008344,  0.10587475,  0.00958745,\n",
      "        -0.23822401, -0.24317302,  0.11823022,  0.04058006,  0.1700793 ,\n",
      "         0.07528265,  0.23476955, -0.10343964, -0.10098625,  0.12493572,\n",
      "         0.05252465,  0.2300843 , -0.12642296,  0.13931645,  0.2283791 ,\n",
      "         0.18142319, -0.10107463,  0.14616202,  0.09827482, -0.12454986,\n",
      "         0.19607799,  0.2878116 , -0.1203149 ,  0.14545079, -0.0665287 ,\n",
      "         0.02678583,  0.07405458, -0.20533861, -0.05817715,  0.09887679],\n",
      "       [-0.09341691, -0.02204679, -0.01336667, -0.05185341, -0.03783506,\n",
      "        -0.2537115 ,  0.31377172,  0.10100466, -0.01438937,  0.10670243,\n",
      "         0.14186817, -0.02945638, -0.01063456,  0.15304224, -0.09169521,\n",
      "        -0.09506288, -0.01851797, -0.00162606,  0.15533341, -0.23337114,\n",
      "         0.05114499, -0.3328518 ,  0.11695296, -0.07906775, -0.19866335,\n",
      "         0.14944878,  0.16126879, -0.05082074, -0.09015757,  0.01663019,\n",
      "        -0.23243953, -0.09995224, -0.27349874,  0.12139165, -0.1573304 ,\n",
      "         0.1859413 ,  0.03964352, -0.35953742, -0.14018498, -0.23136924,\n",
      "        -0.18791683,  0.25821677, -0.13164595, -0.17603293,  0.10899211,\n",
      "         0.0322048 ,  0.01624292, -0.09234285, -0.32571322,  0.01065181,\n",
      "        -0.21605833, -0.21385129,  0.08833855,  0.15621997,  0.15196106,\n",
      "         0.00579957, -0.07264816,  0.16014247, -0.01195533,  0.01134532,\n",
      "        -0.08740831, -0.08815905,  0.17614403,  0.03892466, -0.08747426,\n",
      "        -0.2183918 , -0.31827632,  0.01663362, -0.04281157, -0.0209895 ,\n",
      "        -0.20138252, -0.03287484, -0.15037431, -0.08030426, -0.19482416,\n",
      "         0.05019655,  0.25577676,  0.06598585, -0.09845009, -0.12134383,\n",
      "         0.22000663,  0.11192226, -0.15219483,  0.09533675,  0.10711096,\n",
      "         0.04886432,  0.05239672,  0.12143581, -0.05005921,  0.25977182,\n",
      "         0.14218548,  0.14122778,  0.26652747, -0.11680905, -0.02817893,\n",
      "        -0.26620182, -0.00523943, -0.19077651, -0.07366656,  0.17879216,\n",
      "        -0.15106873, -0.12666316,  0.00665797, -0.04560632, -0.02274907,\n",
      "         0.19617982,  0.02866979,  0.02174085, -0.07555549,  0.05626287,\n",
      "         0.10321458, -0.03375759, -0.20403174,  0.18950994,  0.00536122,\n",
      "        -0.16217619, -0.05992774, -0.06713552,  0.00900272,  0.26527268,\n",
      "        -0.10003526, -0.30235788, -0.04613778, -0.09998863,  0.23958562,\n",
      "         0.09964437,  0.08079442, -0.14200369,  0.10840053,  0.03320989,\n",
      "         0.01920407,  0.21623799, -0.07968483,  0.02969268,  0.19814491,\n",
      "         0.18992761, -0.05939594,  0.17062831, -0.09640701, -0.18242392,\n",
      "         0.15828742,  0.08928611, -0.1124398 , -0.04005485, -0.04429249,\n",
      "         0.1145151 ,  0.02390379, -0.20547193, -0.08278343,  0.18454057],\n",
      "       [-0.11686427, -0.04965051,  0.10302008, -0.06809665,  0.00071735,\n",
      "        -0.19488876,  0.21281943, -0.09653952, -0.0247894 , -0.00997612,\n",
      "         0.14861616, -0.0233098 , -0.01240325,  0.17529589, -0.21117756,\n",
      "        -0.05886437, -0.03083362, -0.06854895,  0.12342651, -0.1313558 ,\n",
      "         0.09857809, -0.25967643,  0.13006887, -0.07794704, -0.12290061,\n",
      "         0.0577496 ,  0.1132417 ,  0.11608662, -0.03595559,  0.07921197,\n",
      "        -0.07926611, -0.0765367 , -0.25674188,  0.07494789, -0.07648726,\n",
      "         0.14067866, -0.01841133, -0.2292018 , -0.07967553, -0.09756926,\n",
      "        -0.18748413,  0.0285757 , -0.27994505, -0.23287468,  0.04971807,\n",
      "         0.00692652,  0.02996042, -0.08875722, -0.30254328,  0.16284922,\n",
      "        -0.24954602, -0.21541217, -0.02165475,  0.0661132 ,  0.08454916,\n",
      "        -0.07705314, -0.05453578,  0.06993009,  0.05780356,  0.05738781,\n",
      "        -0.11849325, -0.06853054,  0.19345132, -0.05876198, -0.04749519,\n",
      "        -0.14053224, -0.16232635,  0.06185275, -0.08591708, -0.01725975,\n",
      "         0.15251118, -0.08588251, -0.03054507, -0.02103803, -0.0229402 ,\n",
      "         0.07228874,  0.17256323,  0.08316102, -0.08694188, -0.06330901,\n",
      "         0.03520669,  0.04334334, -0.03353268,  0.06812354,  0.16204575,\n",
      "         0.0926115 ,  0.12651823,  0.03072288, -0.17535892,  0.21868749,\n",
      "         0.16237135,  0.10387632,  0.11392864, -0.14839664, -0.04379246,\n",
      "        -0.19132088, -0.03469962, -0.18926132, -0.01265807,  0.18155824,\n",
      "        -0.1508857 , -0.00179172, -0.02352421, -0.02724442, -0.07717565,\n",
      "         0.19926262,  0.11731618,  0.11028367,  0.04731885,  0.12660404,\n",
      "        -0.02088271, -0.00696997, -0.21418639,  0.14258327,  0.0188272 ,\n",
      "        -0.11230786, -0.0243813 , -0.0531787 ,  0.05675026,  0.12046268,\n",
      "        -0.08954769, -0.23959093,  0.08032088,  0.10304336,  0.24651857,\n",
      "        -0.01747376,  0.17538436,  0.02824389,  0.0374544 ,  0.06974892,\n",
      "        -0.01029445,  0.10414721, -0.18177718,  0.13251415,  0.16263764,\n",
      "         0.19160937, -0.07880716,  0.14487718, -0.02790656, -0.02342877,\n",
      "         0.02874718,  0.19921784, -0.07118773,  0.09078103, -0.0388707 ,\n",
      "         0.03986516, -0.07655551, -0.17025821,  0.08819171,  0.19377837],\n",
      "       [-0.1632296 ,  0.08173542,  0.00935752, -0.0707539 , -0.04769257,\n",
      "        -0.16212851,  0.23017937,  0.00431193, -0.06330052,  0.0722805 ,\n",
      "         0.15699907, -0.02404654,  0.00954887,  0.06794412, -0.13768147,\n",
      "        -0.03030567, -0.04692348, -0.1797238 ,  0.13945448, -0.18255349,\n",
      "         0.15433595, -0.23789069,  0.15828398, -0.14133824, -0.06447547,\n",
      "         0.09970186,  0.26500344,  0.04624463, -0.03979375,  0.03429389,\n",
      "        -0.2046698 , -0.06166695, -0.33162788, -0.02812312, -0.12006396,\n",
      "         0.22420311,  0.20721774, -0.31377298, -0.08779114, -0.14170139,\n",
      "        -0.20237653,  0.17447105, -0.24169528, -0.22059491,  0.03047255,\n",
      "         0.01281508, -0.11057384,  0.06882744, -0.34221134,  0.2086936 ,\n",
      "        -0.10719771, -0.10426181, -0.10165123,  0.02558908,  0.14918725,\n",
      "        -0.09848305, -0.03040915,  0.22053967, -0.00609182,  0.03843044,\n",
      "        -0.23240678, -0.06389127,  0.2603524 ,  0.02428266, -0.08396145,\n",
      "        -0.3230271 , -0.22927965, -0.19620717, -0.17253678,  0.00780481,\n",
      "        -0.09048279, -0.14487556, -0.07328455, -0.05842801, -0.03268567,\n",
      "        -0.03742499,  0.3041692 ,  0.18830861, -0.05871946,  0.03620787,\n",
      "         0.0101697 ,  0.05122135, -0.01403474,  0.14618815,  0.07320475,\n",
      "         0.11503564,  0.01819476,  0.01576541, -0.18259378,  0.3321589 ,\n",
      "         0.10077825,  0.07582009,  0.16600564, -0.04662775, -0.09332701,\n",
      "        -0.24893701, -0.12160718, -0.26375335, -0.00713576,  0.24249677,\n",
      "        -0.02706736,  0.05489917, -0.0564015 , -0.20213562,  0.1073288 ,\n",
      "         0.18763885,  0.07517257,  0.09200213,  0.10532749,  0.12285032,\n",
      "         0.14385867,  0.06692893, -0.21035029,  0.1956989 , -0.06336821,\n",
      "        -0.11425663,  0.01248354, -0.14702716,  0.14476179,  0.14264347,\n",
      "        -0.17594533, -0.25193503,  0.05271172, -0.03376397,  0.08697909,\n",
      "         0.06107608,  0.07397432, -0.07225789,  0.13938539,  0.0803856 ,\n",
      "         0.05464533,  0.04004806,  0.0896377 ,  0.25114378,  0.18805717,\n",
      "         0.2637804 , -0.09830751,  0.1359985 ,  0.086509  , -0.22444193,\n",
      "         0.26415578,  0.24937676, -0.13801055,  0.17870057,  0.01286454,\n",
      "        -0.05139179, -0.09289071, -0.19376224, -0.05261904, -0.01412435]],\n",
      "      dtype=float32)),)\n"
     ]
    }
   ],
   "source": [
    "#Testing Seq2Seq\n",
    "config ={'cell_type': 'lstm',\n",
    "         'hidden_units': 64 ,\n",
    "         'depth': 2,\n",
    "         'attention_type': 'bahdanou',\n",
    "          'embedding_size': 50,\n",
    "           'use_residual': True,\n",
    "          'attn_input_feeding': False ,\n",
    "           'use_dropout': True,\n",
    "        'dropout_rate' : 0.3,\n",
    "        'optimizer' : 'Adam',\n",
    "        'learning_rate' : 0.001,\n",
    "        'max_gradient_norm': 1.0,\n",
    "        'use_float16': False,\n",
    "        'beam_width': 3,\n",
    "        'max_decode_sep': 18 }\n",
    "\n",
    "\n",
    "obj = Seq2SeqModel('train',1,150,False)\n",
    "obj.build_model()\n",
    "obj.build_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
