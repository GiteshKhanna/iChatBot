{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "from tensorflow.python.ops.rnn_cell import GRUCell\n",
    "from tensorflow.python.ops.rnn_cell import LSTMCell\n",
    "from tensorflow.python.ops.rnn_cell import MultiRNNCell\n",
    "from tensorflow.python.ops.rnn_cell import DropoutWrapper, ResidualWrapper\n",
    "\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.util import nest\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.contrib.seq2seq.python.ops import attention_wrapper\n",
    "from tensorflow.contrib.seq2seq.python.ops import beam_search_decoder\n",
    "\n",
    "from preprocess import *\n",
    "from loading_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resetter\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding parameters\n",
    "embedding_size = 50\n",
    "\n",
    "#data parameters\n",
    "eMax_allowed_length = 64\n",
    "dMax_allowed_length = 20\n",
    "\n",
    "#Model Parameters\n",
    "model_dir = 'tmp/Generation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching data\n",
    "#default directory: 'data/data_10.csv'\n",
    "X,Y= read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching glove vectors\n",
    "#default directory: \"./glove.6B.50d.txt\"\n",
    "embedding_size = 50\n",
    "wi,iw,wv = read_glove_vecs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding extra tokens to glove dictionary\n",
    "go_index,eos_index,unk_index = add_extra_to_dict(wi,iw,wv,embedding_size)\n",
    "emb = map_dict_to_list(iw,wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[148907, 268045,  51581, 185456, 390138, 222470, 360914, 357151,\n",
       "        394474, 151348, 162070, 239104, 270805, 360914, 191914, 254553,\n",
       "        185456,  52942, 379374, 185456,  52942, 154322, 252805, 185456,\n",
       "        174641, 127490, 254257, 354699, 188480, 340711, 189383, 105013,\n",
       "        268045, 137409,  54717, 354661, 388710,  34618, 185456, 105624,\n",
       "        360914, 254257, 144714, 188838, 239104, 384373,  58996, 149238,\n",
       "        240687, 254257, 145582, 383513, 126980, 145182,  54717, 254257,\n",
       "        250481, 192972, 180882, 231487, 264936, 185456, 105624, 400001],\n",
       "       [185456,  52942, 261928, 198880, 185456,  52942, 154322, 295299,\n",
       "        185456, 174641, 106327, 165582, 154322, 146973, 105013,  54717,\n",
       "        289454, 165582, 188480, 135217, 321703, 154322, 357265, 372265,\n",
       "        268045,  44607, 254257, 344584, 185456, 390138, 222470, 360914,\n",
       "        241107, 357211, 185456,  52942, 295271,  44607, 359888,  54717,\n",
       "        254257, 389835, 297631, 219111,  54717, 223829,  77161, 249713,\n",
       "        122127, 271472, 281903, 332014,  72181, 122127, 271472, 281903,\n",
       "        192972, 254257, 384398,  60664, 193715, 243976, 351934, 400001],\n",
       "       [254257, 256045, 192972, 337542, 185456,  52942, 154322,  71420,\n",
       "        178968, 290485, 125828,  75993, 220869, 239104, 355344,  44607,\n",
       "        254257, 297094, 185456,  52942,  85722, 165577, 188480, 106517,\n",
       "        321703,  54717, 185456, 123516, 124652, 154322, 172206,  52317,\n",
       "        388710, 357211, 185456, 277999, 188480, 233707, 143108, 110547,\n",
       "         54717, 293157, 185456, 174641, 127490, 189430, 363600, 188480,\n",
       "        283735, 154322,  96648,  54717, 254257, 145849, 345908, 192972,\n",
       "        185456, 352213,  44607, 254257, 144714, 185456, 174641, 400001],\n",
       "       [254257, 256045, 192972, 299328, 213596, 276311, 185456,  52942,\n",
       "        154322, 197394, 185456, 174641, 127490, 254257,  65631, 268045,\n",
       "        137409, 188480, 135113, 135217, 188480,  17723, 388710, 357265,\n",
       "        125763, 268045, 154322, 354650, 190786, 268045, 354661,  75203,\n",
       "        185456, 174641, 269952, 393301, 142807, 188480, 355088, 189461,\n",
       "        389882,  60664,  43009, 298758, 153799, 137405, 254257, 389835,\n",
       "        383513, 259584, 356584, 254257, 344578, 192972, 185456,  52942,\n",
       "         43009, 297631, 219111, 300611, 231457,  43009, 304965, 400001],\n",
       "       [148907, 268045,  51581, 357160, 151348, 162070, 239104,  43009,\n",
       "         96601, 254257, 256045, 192972, 330207, 246099, 185456,  52942,\n",
       "        154322, 227095, 224020,  62064, 333093, 185456, 174641, 106327,\n",
       "        254257, 188480, 106517, 321703, 154322,  44125, 201317, 354616,\n",
       "        372265, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
       "        400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
       "        400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
       "        400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001],\n",
       "       [ 44607, 254257, 144714, 357639,  58996, 149238, 240687, 188480,\n",
       "        254257, 144714, 254257, 145582, 246099, 175198, 192972,  54272,\n",
       "        138013, 254257, 250481, 246099, 327863, 192972,  43009, 354428,\n",
       "        185456, 174641, 368320,  85004,  54717, 358028, 192972, 269952,\n",
       "        400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
       "        400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
       "        400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001,\n",
       "        400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocessing data\n",
    "#Mapping each word in a sentence to its glove index\n",
    "eInput,eLengths = fit_encoder_text(data= X[1:],word_to_index = wi,max_allowed_seq_length = eMax_allowed_length)\n",
    "dInput,dOutput,dLengths = fit_decoder_text(data= Y[1:],word_to_index = wi,max_allowed_seq_length = dMax_allowed_length)\n",
    "eInput = np.array(eInput)\n",
    "eLengths = np.array(eLengths)\n",
    "dInput = np.array(dInput)\n",
    "dOutput = np.array(dOutput)\n",
    "dLengths = np.array(dLengths)\n",
    "eInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel():\n",
    "    def __init__(self,config,mode):\n",
    "        assert mode.lower() in ['train','decode']\n",
    "        \n",
    "        self.mode = mode.lower()\n",
    "        \n",
    "        #num_encoder_symbols and num_decoder_symbols\n",
    "        self.encoder_vocab_size = 400003\n",
    "        self.decoder_vocab_size = 400003\n",
    "\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        \n",
    "        self.cell_type = config['cell_type']\n",
    "        self.hidden_units = config['hidden_units']\n",
    "        self.depth = config['depth']\n",
    "        self.attention_type = config['attention_type']\n",
    "        self.embedding_size = config['embedding_size']\n",
    "        \n",
    "        self.use_residual = config['use_residual']\n",
    "        self.attn_input_feeding = config['attn_input_feeding']\n",
    "        self.use_dropout = config['use_dropout']\n",
    "        self.keep_prob = 1.0 - config['dropout_rate']\n",
    "        \n",
    "        self.optimizer = config['optimizer']\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.max_gradient_norm = config['max_gradient_norm']\n",
    "        self.global_step = tf.Variable(0, trainable = False, name = 'global_step')\n",
    "        self.global_epoch_step = tf.Variable(0,trainable=False, name = \"global_epoch_step\")\n",
    "        self.global_epoch_step_op= tf.assign(self.global_epoch_step,self.global_epoch_step+1)\n",
    "        \n",
    "        self.dtype = tf.float16 if config['use_float16'] else tf.float32\n",
    "        self.keep_prob_placeholder = tf.placeholder(self.dtype, shape=[], name = 'keep_prob')\n",
    "        \n",
    "        self.use_beamsearch_decode = False\n",
    "        if self.mode == 'decode':\n",
    "            self.beam_width = config['beam_width']\n",
    "            self.use_beamsearch_decode = True if self.beam_width > 1 else False\n",
    "            self.max_decode_step = config['max_decode_step']\n",
    "        \n",
    "        self.build_model()\n",
    "    \n",
    "    def build_model(self):\n",
    "            print('building model..')\n",
    "\n",
    "            #building encoder and decoder networks\n",
    "            self.init_placeholders()\n",
    "            \n",
    "            self.build_encoder()\n",
    "            self.build_decoder()\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "               \n",
    "    def init_placeholders(self):\n",
    "            #encoder inputs: [batch_size, max_time_steps]\n",
    "            self.encoder_inputs = tf.placeholder(dtype = tf.int32, shape = (None,None), name = 'encoder_inputs')\n",
    "            #encoder_inputs_length: [batch_size]\n",
    "            self.encoder_inputs_length = tf.placeholder(dtype=tf.int32, shape=(None,) , name = 'encoder_inputs_length')\n",
    "            \n",
    "            #get dynamic batch_size\n",
    "            self.batch_size = tf.shape(self.encoder_inputs)[0]\n",
    "            \n",
    "            if(self.mode=='train'):\n",
    "                \n",
    "                #decoder_inputs: [batch_size,max_time_steps]\n",
    "                self.decoder_inputs = tf.placeholder(dtype=tf.int32,shape=(None,None), name ='decoder_inputs')\n",
    "                #decoder_inputs_length: [batch_size]\n",
    "                self.decoder_inputs_length = tf.placeholder(dtype=tf.int32, shape=(None,), name='decoder_inputs_length')\n",
    "                \n",
    "                self.decoder_targets = tf.placeholder(dtype=tf.int32,shape=(None,None), name ='decoder_targets')\n",
    "                \n",
    "                '''\n",
    "                #No need, already preprocessed\n",
    "                decoder_start_token=tf.ones(shape=[self.batch_size,1], dtype=tf.int32)*data_utils.start_token\n",
    "                \n",
    "                decoder_end_token=tf.ones(shape=[self.batch_size,1], dtype=tf.int32)*data_utils.end_token\n",
    "                '''\n",
    "                \n",
    "    def build_single_cell(self):\n",
    "        cell_type = LSTMCell\n",
    "        if(self.cell_type.lower() == 'gru'):\n",
    "            cell_type = GRUCell\n",
    "        cell = cell_type(self.hidden_units)\n",
    "        \n",
    "        if self.use_dropout:\n",
    "            cell = DropoutWrapper(cell,dtype=self.dtype,\n",
    "                                 output_keep_prob = self.keep_prob_placeholder)\n",
    "            \n",
    "        if self.use_residual:\n",
    "            cell = ResidualWrapper(cell)\n",
    "            \n",
    "        return cell\n",
    "\n",
    "    def build_encoder_cell (self):\n",
    "        return MultiRNNCell([self.build_single_cell() for i in range(self.depth)])\n",
    "    \n",
    "    def build_decoder_cell(self):\n",
    "        encoder_outputs = self.encoder_outputs\n",
    "        encoder_last_state = self.encoder_last_state\n",
    "        encoder_inputs_length = self.encoder_inputs_length\n",
    "        \n",
    "        if self.use_beamsearch_decode:\n",
    "            print('using beamsearch..')\n",
    "            encoder_outputs = seq2seq.tile_batch(self.encoder_outputs,\n",
    "                                                 multiplier=self.beam_width)\n",
    "            encoder_last_state = nest.map_structure( lambda s: seq2seq.tile_batch(s,self.beam_width),\n",
    "                                                   self.encoder_last_state)\n",
    "            encoder_inputs_length = seq2seq.tile_batch(self.encoder_inputs_length,\n",
    "                                                       multiplier=self.beam_width)\n",
    "            \n",
    "            \n",
    "        #Building attention mechanism: Default Bahdanau\n",
    "        #'Bahdanau' style attention\n",
    "        self.attention_mechanism = attention_wrapper.BahdanauAttention(\n",
    "        num_units=self.hidden_units, memory=encoder_outputs,\n",
    "        memory_sequence_length=encoder_inputs_length,\n",
    "        name='BahdanauAttention')\n",
    "        \n",
    "        # 'Luong' style attention:\n",
    "        if self.attention_type.lower() == 'luong':\n",
    "            self.attention_mechanism = attention_wrapper.LuongAttention(\n",
    "            num_units = self.hidden_units, memory=encoder_outputs,\n",
    "            memory_sequence_length=encoder_inputs_length,\n",
    "            name='LuongAttention')\n",
    "                \n",
    "        #Building decoder_cell\n",
    "        self.decoder_cell_list = [self.build_single_cell() for i in range(self.depth)]\n",
    "        decoder_initial_state = encoder_last_state\n",
    "        \n",
    "        def attn_decoder_input_fn(inputs,attention):\n",
    "            if not self.attn_input_feeding:\n",
    "                return inputs\n",
    "            \n",
    "            _input_layer = Dense(self.hidden_units,dtype = self.dtype,\n",
    "                                name = 'attn_input_feeding')\n",
    "            return _input_layer(array_ops.concat([inputs,attention],-1))\n",
    "        \n",
    "        self.decoder_cell_list[-1] = attention_wrapper.AttentionWrapper(\n",
    "        cell = self.decoder_cell_list[-1],\n",
    "        attention_mechanism=self.attention_mechanism,\n",
    "        attention_layer_size=self.hidden_units,\n",
    "        cell_input_fn=attn_decoder_input_fn,\n",
    "        initial_cell_state=encoder_last_state[-1],\n",
    "        alignment_history=False,\n",
    "        name='Attention_Wrapper')\n",
    "        \n",
    "        # Encoder last state must be compatible with AttentionWrapper\n",
    "        #Attentionwrapper.zero_state is used for the purpose\n",
    "        \n",
    "        batch_size = self.batch_size if not self.use_beamsearch_decode else self.batch_size*self.beam_width\n",
    "        initial_state = [state for state in encoder_last_state]\n",
    "        \n",
    "        initial_state[-1]= self.decoder_cell_list[-1].zero_state(\n",
    "        batch_size = batch_size, dtype=self.dtype)\n",
    "        decoder_initial_state = tuple(initial_state)\n",
    "        \n",
    "        return MultiRNNCell(self.decoder_cell_list),decoder_initial_state\n",
    "        \n",
    "    \n",
    "    def init_optimizer(self):\n",
    "        print(\"Setting optimizer..\")\n",
    "        #Gradients and SGD update operaton for training the model\n",
    "        trainable_params = tf.trainable_variables()\n",
    "        if self.optimizer.lower() == 'adadelta':\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate = self.learning_rate)\n",
    "        elif self.optimizer.lower() == 'adam':\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate = self.learning_rate)\n",
    "        elif self.optimizer.lower() == 'rmsprop':\n",
    "            self.opt = tf.train.RMSPropOptimizer(learning_rate = self.learning_rate)\n",
    "        else:\n",
    "            self.opt = tf.train.GradientDescentOptimizer(learning_rate = self.learning_rate)\n",
    "            \n",
    "        \n",
    "        #Compute gradients of loss w.r.t all trainable variables\n",
    "        gradients = tf.gradients(self.loss,trainable_params)\n",
    "        \n",
    "        #Clip gradients of loss w.r.t all trainable variables\n",
    "        clip_gradients,_ = tf.clip_by_global_norm(gradients,self.max_gradient_norm)\n",
    "        \n",
    "        #Update the model\n",
    "        self.updates = self.opt.apply_gradients(zip(clip_gradients,trainable_params),\n",
    "                                                global_step = self.global_step)\n",
    "        \n",
    "    def save(self,sess,path,var_list=None,global_step=None):\n",
    "        saver = tf.train.Saver(var_list)\n",
    "        \n",
    "        save_path = saver.save(sess,save_path=path,global_step=global_step)\n",
    "        print('model saved at ',save_path)\n",
    "        \n",
    "    def restore(self,sess,path,var_list=None):\n",
    "        saver = tf.train.Saver(var_list)\n",
    "        saver.restore(sess, save_path = path)\n",
    "        print('model restored from ',path)\n",
    "    \n",
    "    def build_encoder(self):\n",
    "        print('Building Encoder..')\n",
    "        with tf.variable_scope('encoder'):\n",
    "            self.encoder_cell = self.build_encoder_cell()\n",
    "            \n",
    "            #Instantiating pretrained embeddings\n",
    "            embedding_variable = tf.Variable(tf.constant(0.0, shape = [self.encoder_vocab_size, embedding_size]),trainable = False, name = 'embedding')\n",
    "                           \n",
    "            self.encoder_embedding_placeholder = tf.placeholder(tf.float32, shape=[self.encoder_vocab_size,embedding_size], name = 'encoder_embedding_placeholder' )\n",
    "            self.encoder_embeddings = embedding_variable.assign(self.encoder_embedding_placeholder)\n",
    "            self.encoder_inputs_embedded=tf.nn.embedding_lookup(self.encoder_embeddings,self.encoder_inputs)\n",
    "            \n",
    "    \n",
    "            #instantiating dense layer\n",
    "            input_layer = Dense(self.hidden_units, dtype = self.dtype, name = 'input_projection')\n",
    "            #passing the embedding through dense layer\n",
    "            self.encoder_inputs_embedded = input_layer(self.encoder_inputs_embedded)\n",
    "            \n",
    "            #Encode input sequences into context vectors\n",
    "            #encoder_outputs: [batch_size, max_time_step, cell_output_size]\n",
    "            #encoder_state: [batch_size,cell_output_size]\n",
    "            self.encoder_outputs, self.encoder_last_state = tf.nn.dynamic_rnn(cell = self.encoder_cell,\n",
    "                                                                               inputs=self.encoder_inputs_embedded,\n",
    "                                                                               sequence_length=self.encoder_inputs_length,\n",
    "                                                                               dtype=self.dtype,\n",
    "                                                                               time_major=False)\n",
    "            \n",
    "            '''\n",
    "            init = tf.global_variables_initializer()\n",
    "            with tf.Session() as sess:\n",
    "                sess.run(init)\n",
    "                enc_outputs,enc_laststate=sess.run([self.encoder_outputs,self.encoder_last_state], \n",
    "                                                   feed_dict={self.encoder_embedding_placeholder:emb ,\n",
    "                                                              self.encoder_inputs:eInput, \n",
    "                                                              self.encoder_inputs_length: eLengths })\n",
    "                print('encoder Outputs:',enc_outputs.shape)\n",
    "                print(enc_outputs)\n",
    "                print()\n",
    "                print('Encoder last state:',len(enc_laststate))\n",
    "                print(enc_laststate)\n",
    "            '''\n",
    "        \n",
    "\n",
    "    def build_decoder(self):\n",
    "        print('Building decoder and attention...')\n",
    "        with tf.variable_scope('decoder'):\n",
    "                \n",
    "            #Recheck this code\n",
    "            self.decoder_cell,self.decoder_initial_state = self.build_decoder_cell()\n",
    "            \n",
    "            #Instantiating pretrained embeddings\n",
    "            embedding_variable = tf.Variable(tf.constant(0.0, shape = [self.decoder_vocab_size, embedding_size]),trainable = False, name = 'embedding')\n",
    "\n",
    "            self.decoder_embedding_placeholder = tf.placeholder(tf.float32, shape=[self.decoder_vocab_size,embedding_size], name = 'decoder_embedding_placeholder' )\n",
    "            self.decoder_embeddings = embedding_variable.assign(self.decoder_embedding_placeholder)\n",
    "\n",
    "                \n",
    "            #instantiating dense layer --> DOUBT\n",
    "            input_layer = Dense(self.hidden_units, dtype = self.dtype, name = 'input_projection')\n",
    "                \n",
    "            #Output projection layer to convert cell outputs to logits --> DOUBT\n",
    "            output_layer = Dense(self.decoder_vocab_size,name = \"output_projection\")\n",
    "                \n",
    "            if self.mode == 'train':\n",
    "                #decoder_inputs_embedded: [batch_size,max_time_step,embedding_size]\n",
    "                self.decoder_inputs_embedded = tf.nn.embedding_lookup(self.decoder_embeddings,\n",
    "                                                                           self.decoder_inputs)\n",
    "                    \n",
    "                #Embedded inputs going through projection layer\n",
    "                self.decoder_inputs_embedded=input_layer(self.decoder_inputs_embedded)\n",
    "                    \n",
    "                #Helper to feed inputs for training: read inputs from dense ground truth vectors\n",
    "                training_helper = seq2seq.TrainingHelper(inputs = self.decoder_inputs_embedded,\n",
    "                                                            sequence_length=self.decoder_inputs_length,\n",
    "                                                            time_major=False,\n",
    "                                                            name='training_helper')\n",
    "                training_decoder = seq2seq.BasicDecoder(cell=self.decoder_cell,\n",
    "                                                           helper = training_helper,\n",
    "                                                           initial_state = self.decoder_initial_state,\n",
    "                                                           output_layer = output_layer)\n",
    "                                                           #output_layer = output_layer\n",
    "                    \n",
    "                #Maximum decoder time_steps in current batch\n",
    "                max_decoder_length = tf.reduce_max(self.decoder_inputs_length)\n",
    "                \n",
    "                # decoder_outputs_train: BasicDecoderOutput\n",
    "                #                        namedtuple(rnn_outputs, sample_id)\n",
    "                # decoder_outputs_train.rnn_output: [batch_size, max_time_step + 1, num_decoder_symbols] if output_time_major=False\n",
    "                #                                   [max_time_step + 1, batch_size, num_decoder_symbols] if output_time_major=True\n",
    "                # decoder_outputs_train.sample_id: [batch_size], tf.int32\n",
    "                \n",
    "                (self.decoder_outputs_train, self.decoder_last_state_train,\n",
    "                self.decoder_outputs_length_train) = (seq2seq.dynamic_decode(decoder=training_decoder,\n",
    "                                                                            output_time_major=False,\n",
    "                                                                            impute_finished=True,\n",
    "                                                                            maximum_iterations=max_decoder_length))\n",
    "                \n",
    "                    \n",
    "\n",
    "                \n",
    "    \n",
    "                    \n",
    "                \n",
    "                # More efficient to do the projection on the batch-time-concatenated tensor\n",
    "                # logits_train: [batch_size, max_time_step + 1, num_decoder_symbols]\n",
    "                # self.decoder_logits_train = output_layer(self.decoder_outputs_train.rnn_output)\n",
    "                self.decoder_logits_train = tf.identity(self.decoder_outputs_train.rnn_output) \n",
    "                # Use argmax to extract decoder symbols to emit\n",
    "                self.decoder_pred_train = tf.argmax(self.decoder_logits_train, axis=-1,\n",
    "                                                        name='decoder_pred_train')\n",
    "                    \n",
    "                # masks: masking for valid and padded time steps, [batch_size, max_time_step + 1]\n",
    "                masks = tf.sequence_mask(lengths=self.decoder_inputs_length, \n",
    "                                         maxlen=max_decoder_length, dtype=self.dtype, name='masks')\n",
    "\n",
    "                # Computes per word average cross-entropy over a batch\n",
    "                # Internally calls 'nn_ops.sparse_softmax_cross_entropy_with_logits' by default\n",
    "                self.loss = seq2seq.sequence_loss(logits=self.decoder_logits_train, \n",
    "                                                  targets=self.decoder_targets,\n",
    "                                                  weights=masks,\n",
    "                                                  average_across_timesteps=True,\n",
    "                                                  average_across_batch=True,)\n",
    "                # Training summary for the current batch_loss\n",
    "                    \n",
    "                # Training summary for the current batch_loss\n",
    "                tf.summary.scalar('loss', self.loss)\n",
    "\n",
    "                # Contruct graphs for minimizing loss\n",
    "                self.init_optimizer()\n",
    "                \n",
    "                #return self.decoder_logits_train,self.decoder_pred_train\n",
    "                #The above return can be removed from comment to test.\n",
    "                \n",
    "            \n",
    "            #When decoding. The output of every time step will go as an input\n",
    "            #to the next time step. Similar to a language model.\n",
    "            elif self.mode == 'decode':\n",
    "                \n",
    "                #Must be of the size [batch_size,] --> int32 vector\n",
    "                start_tokens = tf.ones([self.batch_size,],tf.int32)*go_index\n",
    "                #Must be scalar to be passed to greedyEmbeddingHelper\n",
    "                end_token = eos_index\n",
    "                \n",
    "                def embed_and_input_proj(inputs):\n",
    "                    return input_layer(tf.nn.embedding_lookup(self.decoder_embeddings,inputs))\n",
    "                \n",
    "                \n",
    "                if not self.use_beamsearch_decode:\n",
    "                    #Helper to feed inputs for greedy decoding : Uses argmax of the output\n",
    "                    decoding_helper = seq2seq.GreedyEmbeddingHelper(start_tokens=start_tokens,\n",
    "                                                                   end_token=end_token,\n",
    "                                                                   embedding=embed_and_input_proj)\n",
    "                    \n",
    "                    #Basic decoder performs greedy decoding at each time step\n",
    "                    print(\"Building greedy decoder..\")\n",
    "                    inference_decoder = seq2seq.BasicDecoder(cell=self.decoder_cell,\n",
    "                                                            helper = decoding_helper,\n",
    "                                                            initial_state=self.decoder_initial_state,\n",
    "                                                            output_layer=output_layer)\n",
    "                else:\n",
    "                    #Less greedy approach since we see outputs from a few paths.\n",
    "                    print(\"building beamsearch decoder..\")\n",
    "                    inference_decoder = beam_search_decoder.BeamSearchDecoder(cell=self.decoder_cell,\n",
    "                                                                             embedding=embed_and_input_proj,\n",
    "                                                                             start_tokens=start_tokens,\n",
    "                                                                             end_token=end_token,\n",
    "                                                                             initial_state=self.decoder_initial_state,\n",
    "                                                                             beam_width=self.beam_width,\n",
    "                                                                             output_layer=output_layer)\n",
    "                    \n",
    "                (self.decoder_outputs_decode, self.decoder_last_state_decode,\n",
    "                self.decoder_outputs_length_decode) = (seq2seq.dynamic_decode(\n",
    "                decoder=inference_decoder,\n",
    "                output_time_major=False,\n",
    "                #impute_finished=True, #Could be an error\n",
    "                maximum_iterations=self.max_decode_step))\n",
    "                \n",
    "                if not self.use_beamsearch_decode:\n",
    "                    self.decoder_pred_decode = tf.expand_dims(self.decoder_outputs_decode.sample_id,-1)\n",
    "                else:\n",
    "                    self.decoder_pred_decode = self.decoder_outputs_decode.predicted_ids\n",
    "                    \n",
    "                return self.decoder_pred_decode\n",
    "        \n",
    "    def check_feeds(self,encoder_inputs, encoder_inputs_length,\n",
    "                    decoder_inputs,decoder_targets,\n",
    "                    decoder_inputs_length,decode):\n",
    "        \n",
    "        input_batch_size = encoder_inputs.shape[0]\n",
    "            \n",
    "        if(input_batch_size!=encoder_inputs_length.shape[0]):\n",
    "            raise ValueError(\"Encoder inputs and their lengths must be equal in a batch\",\n",
    "                            input_batch_size,\"!=\",encoder_inputs.shape[0])\n",
    "                \n",
    "        if not decode:\n",
    "            target_batch_size = decoder_inputs.shape[0]\n",
    "            if target_batch_size != input_batch_size:\n",
    "                raise ValueError(\"Encoder inputs and decoder inputs must be equal in their batch size\",\n",
    "                                input_batch_size,\"!=\",target_batch_size)\n",
    "                    \n",
    "            if(target_batch_size!= decoder_inputs_length.shape[0]):\n",
    "                raise ValueError(\"Decoder inputs and their lengths must be equal in a batch\",\n",
    "                                decoder_inputs_length.shape[0],\"!=\",target_batch_size)\n",
    "                    \n",
    "        input_feed = {}\n",
    "            \n",
    "        input_feed[self.encoder_inputs.name] = encoder_inputs\n",
    "        input_feed[self.encoder_inputs_length.name] = encoder_inputs_length\n",
    "            \n",
    "        if not decode:\n",
    "            input_feed[self.decoder_inputs.name] = decoder_inputs\n",
    "            input_feed[self.decoder_targets.name] = decoder_targets\n",
    "            input_feed[self.decoder_inputs_length.name] = decoder_inputs_length\n",
    "                \n",
    "        return input_feed\n",
    "            \n",
    "            \n",
    "    def train(self,sess,encoder_embedding,decoder_embedding\n",
    "              ,encoder_inputs,encoder_inputs_length,\n",
    "             decoder_inputs,decoder_targets,\n",
    "              decoder_inputs_length):\n",
    "            \n",
    "        \n",
    "        if self.mode.lower()!='train':\n",
    "            raise ValueError(\"Mode Conflict.Choose Train mode to continue.\")\n",
    "                \n",
    "        input_feed = self.check_feeds(encoder_inputs, encoder_inputs_length,\n",
    "                                      decoder_inputs,decoder_targets, decoder_inputs_length, False)\n",
    "            \n",
    "        #Input feeds for dropout\n",
    "        input_feed[self.keep_prob_placeholder.name] = self.keep_prob\n",
    "        \n",
    "        #input feed for embedding placeholders\n",
    "        input_feed[self.encoder_embedding_placeholder.name] = encoder_embedding\n",
    "        input_feed[self.decoder_embedding_placeholder.name] = decoder_embedding\n",
    "            \n",
    "        output_feed = [self.updates,\n",
    "                          self.loss,\n",
    "                          self.summary_op]\n",
    "            \n",
    "        outputs = sess.run(output_feed,input_feed)\n",
    "        return outputs[1], outputs[2]\n",
    "        \n",
    "        \n",
    "    def eval(self, sess, encoder_inputs, encoder_inputs_length,\n",
    "            decoder_inputs,decoder_inputs_length):\n",
    "            \n",
    "        input_feed = self.check_feeds(encoder_inputs, encoder_inputs_length,\n",
    "                                         decoder_inputs,decoder_inputs_length,False)\n",
    "            \n",
    "        #input feeds for dropout\n",
    "        input_feed[self.keep_prob_placeholder.name] = 1.0\n",
    "        \n",
    "        #input feed for embedding placeholders\n",
    "        input_feed[self.encoder_embedding_placeholder.name] = encoder_embedding\n",
    "        input_feed[self.decoder_embedding_placeholder.name] = decoder_embedding\n",
    "            \n",
    "        output_feed = [self.loss,\n",
    "                       self.summary_op]\n",
    "            \n",
    "        outputs = sess.run(outputs_feed, input_feed)\n",
    "        return outputs[0], outputs[1]\n",
    "        \n",
    "    def predict(self,sess,encoder_embedding,decoder_embedding,encoder_inputs, encoder_inputs_length):\n",
    "            \n",
    "        input_feed = self.check_feeds(encoder_inputs, encoder_inputs_length,\n",
    "                                      decoder_inputs=None,decoder_targets=None, decoder_inputs_length=None,\n",
    "                                      decode=True)\n",
    "            \n",
    "        #Input feeds for dropout\n",
    "        input_feed[self.keep_prob_placeholder.name] = 1.0\n",
    "        \n",
    "        #input feed for embedding placeholders\n",
    "        input_feed[self.encoder_embedding_placeholder.name] = encoder_embedding\n",
    "        input_feed[self.decoder_embedding_placeholder.name] = decoder_embedding\n",
    "            \n",
    "        output_feed = [self.decoder_pred_decode]\n",
    "        outputs = sess.run(output_feed, input_feed)\n",
    "            \n",
    "        #GreedyDecoder: [batch_size, max_time_steps]\n",
    "        #BeamSearchDecoder: [batch_size, max_time_steps * beam_width]\n",
    "        return outputs[0]\n",
    "            \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n@Test instruction: open return options in encoder and decoder function\\n                   build_model() function must be commented so that it does NOT initialize encoder & decoder\\n\\n                   \\n#Testing Seq2Seq\\nreset_graph()\\n\\nconfig ={\\'cell_type\\': \\'lstm\\',\\n         \\'hidden_units\\': 64 ,\\n         \\'depth\\': 2,\\n         \\'attention_type\\': \\'bahdanou\\',\\n          \\'embedding_size\\': 50,\\n           \\'use_residual\\': True,\\n          \\'attn_input_feeding\\': False ,\\n           \\'use_dropout\\': True,\\n        \\'dropout_rate\\' : 0.3,\\n        \\'optimizer\\' : \\'Adam\\',\\n        \\'learning_rate\\' : 0.001,\\n        \\'max_gradient_norm\\': 1.0,\\n        \\'use_float16\\': False,\\n        \\'beam_width\\': 3,\\n        \\'max_decode_step\\': 18 }\\n\\n\\n\\nphase = \\'decode\\'\\nobj = Seq2SeqModel(config,phase)\\nobj.build_model()\\nobj.build_encoder()\\n\\n#obj.train()\\n#obj.eval()\\n#obj.predict()\\n\\nif(phase==\\'train\\'):\\n    dec_logits,dec_argmax = obj.build_decoder()\\n    \\nif(phase==\\'decode\\'):\\n    decoder_predicted = obj.build_decoder()\\n\\n\\ninit = tf.global_variables_initializer()\\nwith tf.Session() as sess:\\n    sess.run(init)\\n    eie,enc_outputs,enc_laststate=sess.run([obj.encoder_inputs_embedded,obj.encoder_outputs,obj.encoder_last_state], \\n                                                   feed_dict={obj.encoder_embedding_placeholder:emb ,\\n                                                              obj.encoder_inputs:eInput, \\n                                                              obj.encoder_inputs_length: eLengths,\\n                                                              obj.keep_prob_placeholder : 0.3 })\\n    \\n    if(phase==\\'train\\'):\\n        dec_logits,dec_argmax = sess.run([dec_logits,dec_argmax],feed_dict={obj.decoder_embedding_placeholder:emb ,\\n                                                                  obj.decoder_inputs_length: dLengths,\\n                                                                  obj.decoder_targets:dOutput,\\n                                                                  obj.decoder_inputs: dInput,\\n                                                                  obj.encoder_embedding_placeholder:emb,\\n                                                                  obj.encoder_inputs:eInput, \\n                                                                  obj.encoder_inputs_length: eLengths,\\n                                                                  obj.keep_prob_placeholder : 0.3} )\\n    \\n    if(phase==\\'decode\\'):\\n        decoder_predicted = sess.run(decoder_predicted,feed_dict={obj.decoder_embedding_placeholder:emb,\\n                                                                  obj.encoder_embedding_placeholder:emb,\\n                                                                  obj.encoder_inputs:eInput, \\n                                                                  obj.encoder_inputs_length: eLengths,\\n                                                                  obj.keep_prob_placeholder : 0.3})\\n\\n    print(\\'encoder Outputs:\\',enc_outputs.shape)\\n    #print(enc_outputs[0])\\n    print()\\n    #print(\\'Encoder last state:\\',len(enc_laststate))\\n    #print(enc_laststate)\\n    print()\\n    \\n    if(phase==\\'train\\'):\\n        print(\"Decoder Logits:\",dec_logits.shape)\\n        print(dec_logits)\\n        print()\\n        print()\\n        print(\"Decoder Argmax:\",dec_argmax.shape)\\n        print()\\n        print(dec_argmax)\\n        print()\\n    \\n    if(phase==\\'decode\\'):\\n        print(\"Decoder predicted:\",decoder_predicted.shape)\\n        print(decoder_predicted)\\n        print()\\n        \\n        \\n        \\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "@Test instruction: open return options in encoder and decoder function\n",
    "                   build_model() function must be commented so that it does NOT initialize encoder & decoder\n",
    "\n",
    "                   \n",
    "#Testing Seq2Seq\n",
    "reset_graph()\n",
    "\n",
    "config ={'cell_type': 'lstm',\n",
    "         'hidden_units': 64 ,\n",
    "         'depth': 2,\n",
    "         'attention_type': 'bahdanou',\n",
    "          'embedding_size': 50,\n",
    "           'use_residual': True,\n",
    "          'attn_input_feeding': False ,\n",
    "           'use_dropout': True,\n",
    "        'dropout_rate' : 0.3,\n",
    "        'optimizer' : 'Adam',\n",
    "        'learning_rate' : 0.001,\n",
    "        'max_gradient_norm': 1.0,\n",
    "        'use_float16': False,\n",
    "        'beam_width': 3,\n",
    "        'max_decode_step': 18 }\n",
    "\n",
    "\n",
    "\n",
    "phase = 'decode'\n",
    "obj = Seq2SeqModel(config,phase)\n",
    "obj.build_model()\n",
    "obj.build_encoder()\n",
    "\n",
    "#obj.train()\n",
    "#obj.eval()\n",
    "#obj.predict()\n",
    "\n",
    "if(phase=='train'):\n",
    "    dec_logits,dec_argmax = obj.build_decoder()\n",
    "    \n",
    "if(phase=='decode'):\n",
    "    decoder_predicted = obj.build_decoder()\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    eie,enc_outputs,enc_laststate=sess.run([obj.encoder_inputs_embedded,obj.encoder_outputs,obj.encoder_last_state], \n",
    "                                                   feed_dict={obj.encoder_embedding_placeholder:emb ,\n",
    "                                                              obj.encoder_inputs:eInput, \n",
    "                                                              obj.encoder_inputs_length: eLengths,\n",
    "                                                              obj.keep_prob_placeholder : 0.3 })\n",
    "    \n",
    "    if(phase=='train'):\n",
    "        dec_logits,dec_argmax = sess.run([dec_logits,dec_argmax],feed_dict={obj.decoder_embedding_placeholder:emb ,\n",
    "                                                                  obj.decoder_inputs_length: dLengths,\n",
    "                                                                  obj.decoder_targets:dOutput,\n",
    "                                                                  obj.decoder_inputs: dInput,\n",
    "                                                                  obj.encoder_embedding_placeholder:emb,\n",
    "                                                                  obj.encoder_inputs:eInput, \n",
    "                                                                  obj.encoder_inputs_length: eLengths,\n",
    "                                                                  obj.keep_prob_placeholder : 0.3} )\n",
    "    \n",
    "    if(phase=='decode'):\n",
    "        decoder_predicted = sess.run(decoder_predicted,feed_dict={obj.decoder_embedding_placeholder:emb,\n",
    "                                                                  obj.encoder_embedding_placeholder:emb,\n",
    "                                                                  obj.encoder_inputs:eInput, \n",
    "                                                                  obj.encoder_inputs_length: eLengths,\n",
    "                                                                  obj.keep_prob_placeholder : 0.3})\n",
    "\n",
    "    print('encoder Outputs:',enc_outputs.shape)\n",
    "    #print(enc_outputs[0])\n",
    "    print()\n",
    "    #print('Encoder last state:',len(enc_laststate))\n",
    "    #print(enc_laststate)\n",
    "    print()\n",
    "    \n",
    "    if(phase=='train'):\n",
    "        print(\"Decoder Logits:\",dec_logits.shape)\n",
    "        print(dec_logits)\n",
    "        print()\n",
    "        print()\n",
    "        print(\"Decoder Argmax:\",dec_argmax.shape)\n",
    "        print()\n",
    "        print(dec_argmax)\n",
    "        print()\n",
    "    \n",
    "    if(phase=='decode'):\n",
    "        print(\"Decoder predicted:\",decoder_predicted.shape)\n",
    "        print(decoder_predicted)\n",
    "        print()\n",
    "        \n",
    "        \n",
    "        \n",
    "'''        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(session,config,model_dir):\n",
    "    model = Seq2SeqModel(config,'train')\n",
    "    ckpt = tf.train.get_checkpoint_state(model_dir)\n",
    "    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "        print('Reloading model parameters..')\n",
    "        model.restore(session,ckpt.model_checkpoint_path)\n",
    "    \n",
    "    else:\n",
    "        print('Creating new model parameters')\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config,model_dir,model_name,num_epochs):\n",
    "    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        #Create a log writer object\n",
    "        log_writer = tf.summary.FileWriter(model_dir+'tf_logs/',graph=sess.graph)\n",
    "        \n",
    "        #Create a new model or reload existing checkpoint\n",
    "        model_dir = model_dir + 'model/'\n",
    "        model = create_model(sess,config,model_dir)\n",
    "        \n",
    "        step_time,loss = 0.0,0.0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        #Training loop\n",
    "        print('Training..')\n",
    "        for epoch in range(num_epochs):\n",
    "            if model.global_epoch_step.eval() >= num_epochs:\n",
    "                print('Training is already complete'),\n",
    "                'current epoch:{},max epoch:{}'.format(model.global_epoch_step.eval(),\n",
    "                                                       num_epochs)\n",
    "                break\n",
    "                \n",
    "            step_loss,summary = model.train(sess,encoder_embedding = emb,\n",
    "                                           decoder_embedding = emb,\n",
    "                                           encoder_inputs=eInput,\n",
    "                                           encoder_inputs_length=eLengths,\n",
    "                                           decoder_inputs=dInput,decoder_targets=dOutput,\n",
    "                                           decoder_inputs_length=dLengths)\n",
    "            print('Ek scalar')\n",
    "                                           \n",
    "            \n",
    "            display_freq = 5\n",
    "            loss += float(step_loss)/display_freq\n",
    "            \n",
    "            if model.global_step.eval() % display_freq == 0:\n",
    "                time_elapsed = time.time() - start_time\n",
    "                step_time = time_elapsed / display_freq\n",
    "                \n",
    "                print('Epoch:',model.global_epoch_step.eval(),\n",
    "                     'Step:',model.global_step.eval(),\n",
    "                     'Step time:',step_time)\n",
    "                log_writer.add_summary(summary, model.global_step.eval())\n",
    "                \n",
    "            \n",
    "            save_freq = 5\n",
    "            if model.global_step.eval() % save_freq ==5:\n",
    "                print('Saving the model...')\n",
    "                checkpoint_path = os.path.join(model_dir,model_name)\n",
    "                model.save(sess,checkpoint_path,global_step=model.global_step)\n",
    "                \n",
    "            #increase epoch index of model\n",
    "            model.global_epoch_step_op.eval()\n",
    "            print('Epoch {0:} Done'.format(model.global_epoch_step.eval()))\n",
    "                \n",
    "        print('Saving last model..')\n",
    "        checkpoint_path = os.path.join(model_dir,model_name)\n",
    "        model.save(sess,checkpoint_path,global_step=model.global_step)\n",
    "        \n",
    "            \n",
    "            \n",
    "    print('Training Terminated')\n",
    "                \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model..\n",
      "Building Encoder..\n",
      "Building decoder and attention...\n",
      "Setting optimizer..\n",
      "Creating new model parameters\n",
      "Training..\n",
      "Ek scalar\n",
      "Epoch 1 Done\n",
      "Ek scalar\n",
      "Epoch 2 Done\n",
      "Ek scalar\n",
      "Epoch 3 Done\n",
      "Ek scalar\n",
      "Epoch 4 Done\n",
      "Ek scalar\n",
      "Epoch: 4 Step: 5 Step time: 7.18629903793335\n",
      "Epoch 5 Done\n",
      "Ek scalar\n",
      "Epoch 6 Done\n",
      "Ek scalar\n",
      "Epoch 7 Done\n",
      "Ek scalar\n",
      "Epoch 8 Done\n",
      "Ek scalar\n",
      "Epoch 9 Done\n",
      "Ek scalar\n",
      "Epoch: 9 Step: 10 Step time: 14.478324508666992\n",
      "Epoch 10 Done\n",
      "Ek scalar\n",
      "Epoch 11 Done\n",
      "Ek scalar\n",
      "Epoch 12 Done\n",
      "Ek scalar\n",
      "Epoch 13 Done\n",
      "Ek scalar\n",
      "Epoch 14 Done\n",
      "Ek scalar\n",
      "Epoch: 14 Step: 15 Step time: 21.74116630554199\n",
      "Epoch 15 Done\n",
      "Saving last model..\n",
      "model saved at  tmp/Generation/model/LSTM-1.ckpt-15\n",
      "Training Terminated\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "config ={'cell_type': 'lstm',\n",
    "         'hidden_units': 64 ,\n",
    "         'depth': 1,\n",
    "         'attention_type': 'bahdanou',\n",
    "          'embedding_size': 50,\n",
    "           'use_residual': True,\n",
    "          'attn_input_feeding': False ,\n",
    "           'use_dropout': True,\n",
    "        'dropout_rate' : 0.3,\n",
    "        'optimizer' : 'Adam',\n",
    "        'learning_rate' : 0.001,\n",
    "        'max_gradient_norm': 1.0,\n",
    "        'use_float16': False,\n",
    "        'beam_width': 3,\n",
    "        'max_decode_step': 15 }\n",
    "\n",
    "\n",
    "model_name = 'LSTM-1' + '.ckpt'\n",
    "train(config,model_dir,model_name,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating test data\n",
    "X_test,Y_test = read_csv('data/data10_test.csv')\n",
    "eInput_test,eLengths_test = fit_encoder_text(data= X_test[1:],word_to_index = wi,max_allowed_seq_length = eMax_allowed_length)\n",
    "eInput_test= np.array(eInput_test)\n",
    "eLengths_test = np.array(eLengths_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(session, config,model_path):\n",
    "    \n",
    "    model = Seq2SeqModel(config, 'decode')\n",
    "    ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "        print('Reloading model parameters..')\n",
    "        model.restore(session,ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'No such file:[{}]'.format(model_path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model..\n",
      "Building Encoder..\n",
      "Building decoder and attention...\n",
      "using beamsearch..\n",
      "building beamsearch decoder..\n",
      "Reloading model parameters..\n",
      "INFO:tensorflow:Restoring parameters from tmp/Generation/model/LSTM-1.ckpt-15\n",
      "model restored from  tmp/Generation/model/LSTM-1.ckpt-15\n"
     ]
    }
   ],
   "source": [
    "'TESTING: load_model'\n",
    "\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "config ={'cell_type': 'lstm',\n",
    "         'hidden_units': 64 ,\n",
    "         'depth': 1,\n",
    "         'attention_type': 'bahdanou',\n",
    "          'embedding_size': 50,\n",
    "           'use_residual': True,\n",
    "          'attn_input_feeding': False ,\n",
    "           'use_dropout': True,\n",
    "        'dropout_rate' : 0.3,\n",
    "        'optimizer' : 'Adam',\n",
    "        'learning_rate' : 0.001,\n",
    "        'max_gradient_norm': 1.0,\n",
    "        'use_float16': False,\n",
    "        'beam_width': 3,\n",
    "        'max_decode_step': 8 }\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    load_model(sess,config,'tmp/Generation/model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(train_config,decode_config,eInput):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    # Load source data to decode: DECODE_CONFIG\n",
    "    test_set = TextIterator(source=config['decode_input'],\n",
    "                            batch_size=config['decode_batch_size'],\n",
    "                            source_dict=config['source_vocabulary'],\n",
    "                            maxlen=None,\n",
    "                            n_words_source=config['num_encoder_symbols'])\n",
    "    '''\n",
    "    \n",
    "    eInput_test,eLengths_test = eInput\n",
    "    with tf.Session() as sess:\n",
    "        model = load_model(sess,train_config,decode_config['model_path'])\n",
    "        \n",
    "        print('Decoding{}..'.format(decode_config['decode_input']))\n",
    "        for idx,source_seq in enumerate(eInput_test):\n",
    "                predicted_ids = model.predict(sess,encoder_embedding=emb,\n",
    "                                              decoder_embedding=emb,\n",
    "                                              encoder_inputs=eInput_test,\n",
    "                                              encoder_inputs_length=eLengths_test)\n",
    "                for seq in predicted_ids:\n",
    "                    for beam_num in range(train_config['beam_width']):\n",
    "                        print(seq[:,beam_num])\n",
    "                        print()\n",
    "                        print(seq2words(seq[:,beam_num],iw))\n",
    "                        print()\n",
    "                        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model..\n",
      "Building Encoder..\n",
      "Building decoder and attention...\n",
      "using beamsearch..\n",
      "building beamsearch decoder..\n",
      "Reloading model parameters..\n",
      "INFO:tensorflow:Restoring parameters from tmp/Generation/model/LSTM-1.ckpt-15\n",
      "model restored from  tmp/Generation/model/LSTM-1.ckpt-15\n",
      "Decodingdata/data10_test.csv..\n",
      "[ 44607  57169  57169  57169 142807  44607 394474 327481 191291 191291\n",
      " 191291 191291 191291 191291 191291]\n",
      "\n",
      "['about', 'any', 'any', 'any', 'experience', 'about', 'you', 'share', 'interests', 'interests', 'interests', 'interests', 'interests', 'interests', 'interests']\n",
      "\n",
      "\n",
      "[ 44607  57169  57169  57169 142807  44607 394474 191291 191291 191291\n",
      " 191291 191291 191291 191291 191291]\n",
      "\n",
      "['about', 'any', 'any', 'any', 'experience', 'about', 'you', 'interests', 'interests', 'interests', 'interests', 'interests', 'interests', 'interests', 'interests']\n",
      "\n",
      "\n",
      "[ 44607  57169  57169  57169 142807  44607 394474 327481 191291 191291\n",
      " 191291 191291 191291 327481 191291]\n",
      "\n",
      "['about', 'any', 'any', 'any', 'experience', 'about', 'you', 'share', 'interests', 'interests', 'interests', 'interests', 'interests', 'share', 'interests']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "config ={'cell_type': 'lstm',\n",
    "         'hidden_units': 64 ,\n",
    "         'depth': 1,\n",
    "         'attention_type': 'bahdanou',\n",
    "          'embedding_size': 50,\n",
    "           'use_residual': True,\n",
    "          'attn_input_feeding': False ,\n",
    "           'use_dropout': True,\n",
    "        'dropout_rate' : 0.3,\n",
    "        'optimizer' : 'Adam',\n",
    "        'learning_rate' : 0.001,\n",
    "        'max_gradient_norm': 1.0,\n",
    "        'use_float16': False,\n",
    "        'beam_width': 3,\n",
    "        'max_decode_step': 15 }\n",
    "\n",
    "decode_config = {'model_path': 'tmp/Generation/model/',\n",
    "                'decode_input': 'data/data10_test.csv',\n",
    "                'write_n_best': True}                \n",
    "\n",
    "\n",
    "testing_input = (eInput_test,eLengths_test)\n",
    "decode(train_config=config,decode_config=decode_config,eInput=testing_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
