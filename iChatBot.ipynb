{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "\n",
    "from tensorflow.python.ops.rnn_cell import GRUCell\n",
    "from tensorflow.python.ops.rnn_cell import LSTMCell\n",
    "from tensorflow.python.ops.rnn_cell import MultiRNNCell\n",
    "from tensorflow.python.ops.rnn_cell import DropoutWrapper, ResidualWrapper\n",
    "\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.util import nest\n",
    "\n",
    "\n",
    "from tensorflow.contrib.seq2seq.python.ops import attention_wrapper\n",
    "from tensorflow.contrib.seq2seq.python.ops import beam_search_decoder\n",
    "\n",
    "from preprocess import *\n",
    "from loading_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resetter\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding parameters\n",
    "embedding_size = 50\n",
    "\n",
    "#data parameters\n",
    "eMax_allowed_length = 100\n",
    "dMax_allowed_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching data\n",
    "#default directory: 'data/data_10.csv'\n",
    "X,Y= read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching glove vectors\n",
    "#default directory: \"./glove.6B.50d.txt\"\n",
    "embedding_size = 50\n",
    "wi,iw,wv = read_glove_vecs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding extra tokens to glove dictionary\n",
    "go_index,eos_index,unk_index = add_extra_to_dict(wi,iw,wv,embedding_size)\n",
    "emb = map_dict_to_list(iw,wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 19, 13, 18, 11, 18]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocessing data\n",
    "#Mapping each word in a sentence to its glove index\n",
    "eInput,eLengths = fit_encoder_text(data= X[1:],word_to_index = wi,max_allowed_seq_length = eMax_allowed_length)\n",
    "dInput,dOutput,dLengths = fit_decoder_text(data= Y[1:],word_to_index = wi,max_allowed_seq_length = dMax_allowed_length)\n",
    "dLengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel():\n",
    "    def __init__(self,config,mode):\n",
    "        assert mode.lower() in ['train','decode']\n",
    "        \n",
    "        self.mode = mode.lower()\n",
    "        \n",
    "        #num_encoder_symbols and num_decoder_symbols\n",
    "        self.encoder_vocab_size = 400003\n",
    "        self.decoder_vocab_size = 400003\n",
    "\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        \n",
    "        self.cell_type = config['cell_type']\n",
    "        self.hidden_units = config['hidden_units']\n",
    "        self.depth = config['depth']\n",
    "        self.attention_type = config['attention_type']\n",
    "        self.embedding_size = config['embedding_size']\n",
    "        \n",
    "        self.use_residual = config['use_residual']\n",
    "        self.attn_input_feeding = config['attn_input_feeding']\n",
    "        self.use_dropout = config['use_dropout']\n",
    "        self.keep_prob = 1.0 - config['dropout_rate']\n",
    "        \n",
    "        self.optimizer = config['optimizer']\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.max_gradient_norm = config['max_gradient_norm']\n",
    "        self.global_step = tf.Variable(0, trainable = False, name = 'global_step')\n",
    "        self.global_epoch_step = tf.Variable(0,trainable=False, name = \"global_epoch_step\")\n",
    "        self.global_epoch_step_op= tf.assign(self.global_epoch_step,self.global_epoch_step+1)\n",
    "        \n",
    "        self.dtype = tf.float16 if config['use_float16'] else tf.float32\n",
    "        self.keep_prob_placeholder = tf.placeholder(self.dtype, shape=[], name = 'keep_prob')\n",
    "        \n",
    "        self.use_beamsearch_decode = False\n",
    "        if self.mode == 'decode':\n",
    "            self.beam_width = config['beam_width']\n",
    "            self.use_beamsearch_decode = True if self.beam_width > 1 else False\n",
    "            self.max_decode_step = config['max_decode_step']\n",
    "        \n",
    "        self.build_model()\n",
    "    \n",
    "    def build_model(self):\n",
    "            print('building model..')\n",
    "\n",
    "            #building encoder and decoder networks\n",
    "            self.init_placeholders()\n",
    "            '''\n",
    "            self.build_encoder()\n",
    "            self.build_decoder()\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "            '''    \n",
    "    def init_placeholders(self):\n",
    "            #encoder inputs: [batch_size, max_time_steps]\n",
    "            self.encoder_inputs = tf.placeholder(dtype = tf.int32, shape = (None,None), name = 'encoder_inputs')\n",
    "            #encoder_inputs_length: [batch_size]\n",
    "            self.encoder_inputs_length = tf.placeholder(dtype=tf.int32, shape=(None,) , name = 'encoder_inputs_length')\n",
    "            \n",
    "            #get dynamic batch_size\n",
    "            self.batch_size = tf.shape(self.encoder_inputs)[0]\n",
    "            \n",
    "            if(self.mode=='train'):\n",
    "                \n",
    "                #decoder_inputs: [batch_size,max_time_steps]\n",
    "                self.decoder_inputs = tf.placeholder(dtype=tf.int32,shape=(None,None), name ='decoder_inputs')\n",
    "                #decoder_inputs_length: [batch_size]\n",
    "                self.decoder_inputs_length = tf.placeholder(dtype=tf.int32, shape=(None,), name='decoder_inputs_length')\n",
    "                \n",
    "                self.decoder_targets = tf.placeholder(dtype=tf.int32,shape=(None,None), name ='decoder_targets')\n",
    "                \n",
    "                '''\n",
    "                #No need, already preprocessed\n",
    "                decoder_start_token=tf.ones(shape=[self.batch_size,1], dtype=tf.int32)*data_utils.start_token\n",
    "                \n",
    "                decoder_end_token=tf.ones(shape=[self.batch_size,1], dtype=tf.int32)*data_utils.end_token\n",
    "                '''\n",
    "                \n",
    "    def build_single_cell(self):\n",
    "        cell_type = LSTMCell\n",
    "        if(self.cell_type.lower() == 'gru'):\n",
    "            cell_type = GRUCell\n",
    "        cell = cell_type(self.hidden_units)\n",
    "        \n",
    "        if self.use_dropout:\n",
    "            cell = DropoutWrapper(cell,dtype=self.dtype,\n",
    "                                 output_keep_prob = self.keep_prob_placeholder)\n",
    "            \n",
    "        if self.use_residual:\n",
    "            cell = ResidualWrapper(cell)\n",
    "            \n",
    "        return cell\n",
    "\n",
    "    def build_encoder_cell (self):\n",
    "        return MultiRNNCell([self.build_single_cell() for i in range(self.depth)])\n",
    "    \n",
    "    def build_decoder_cell(self):\n",
    "        encoder_outputs = self.encoder_outputs\n",
    "        encoder_last_state = self.encoder_last_state\n",
    "        encoder_inputs_length = self.encoder_inputs_length\n",
    "        \n",
    "        if self.use_beamsearch_decode:\n",
    "            print('using beamsearch..')\n",
    "            encoder_outputs = seq2seq.tile_batch(self.encoder_outputs,\n",
    "                                                 multiplier=self.beam_width)\n",
    "            encoder_last_state = nest.map_structure( lambda s: seq2seq.tile_batch(s,self.beam_width),\n",
    "                                                   self.encoder_last_state)\n",
    "            encoder_inputs_length = seq2seq.tile_batch(self.encoder_inputs_length,\n",
    "                                                       multiplier=self.beam_width)\n",
    "            \n",
    "            \n",
    "        #Building attention mechanism: Default Bahdanau\n",
    "        #'Bahdanau' style attention\n",
    "        self.attention_mechanism = attention_wrapper.BahdanauAttention(\n",
    "        num_units=self.hidden_units, memory=encoder_outputs,\n",
    "        memory_sequence_length=encoder_inputs_length,\n",
    "        name='BahdanauAttention')\n",
    "        \n",
    "        # 'Luong' style attention:\n",
    "        if self.attention_type.lower() == 'luong':\n",
    "            self.attention_mechanism = attention_wrapper.LuongAttention(\n",
    "            num_units = self.hidden_units, memory=encoder_outputs,\n",
    "            memory_sequence_length=encoder_inputs_length,\n",
    "            name='LuongAttention')\n",
    "                \n",
    "        #Building decoder_cell\n",
    "        self.decoder_cell_list = [self.build_single_cell() for i in range(self.depth)]\n",
    "        decoder_initial_state = encoder_last_state\n",
    "        \n",
    "        def attn_decoder_input_fn(inputs,attention):\n",
    "            if not self.attn_input_feeding:\n",
    "                return inputs\n",
    "            \n",
    "            _input_layer = Dense(self.hidden_units,dtype = self.dtype,\n",
    "                                name = 'attn_input_feeding')\n",
    "            return _input_layer(array_ops.concat([inputs,attention],-1))\n",
    "        \n",
    "        self.decoder_cell_list[-1] = attention_wrapper.AttentionWrapper(\n",
    "        cell = self.decoder_cell_list[-1],\n",
    "        attention_mechanism=self.attention_mechanism,\n",
    "        attention_layer_size=self.hidden_units,\n",
    "        cell_input_fn=attn_decoder_input_fn,\n",
    "        initial_cell_state=encoder_last_state[-1],\n",
    "        alignment_history=False,\n",
    "        name='Attention_Wrapper')\n",
    "        \n",
    "        # Encoder last state must be compatible with AttentionWrapper\n",
    "        #Attentionwrapper.zero_state is used for the purpose\n",
    "        \n",
    "        batch_size = self.batch_size if not self.use_beamsearch_decode else self.batch_size*self.beam_width\n",
    "        initial_state = [state for state in encoder_last_state]\n",
    "        \n",
    "        initial_state[-1]= self.decoder_cell_list[-1].zero_state(\n",
    "        batch_size = batch_size, dtype=self.dtype)\n",
    "        decoder_initial_state = tuple(initial_state)\n",
    "        \n",
    "        return MultiRNNCell(self.decoder_cell_list),decoder_initial_state\n",
    "        \n",
    "    \n",
    "    def init_optimizer(self):\n",
    "        print(\"Setting optimizer..\")\n",
    "        #Gradients and SGD update operaton for training the model\n",
    "        trainable_params = tf.trainable_variables()\n",
    "        if self.optimizer.lower() == 'adadelta':\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate = self.learning_rate)\n",
    "        elif self.optimizer.lower() == 'adam':\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate = self.learning_rate)\n",
    "        elif self.optimizer.lower() == 'rmsprop':\n",
    "            self.opt = tf.train.RMSPropOptimizer(learning_rate = self.learning_rate)\n",
    "        else:\n",
    "            self.opt = tf.train.GradientDescentOptimizer(learning_rate = self.learning_rate)\n",
    "            \n",
    "        \n",
    "        #Compute gradients of loss w.r.t all trainable variables\n",
    "        gradients = tf.gradients(self.loss,trainable_params)\n",
    "        \n",
    "        #Clip gradients of loss w.r.t all trainable variables\n",
    "        clip_gradients,_ = tf.clip_by_global_norm(gradients,self.max_gradient_norm)\n",
    "        \n",
    "        #Update the model\n",
    "        self.updates = self.opt.apply_gradients(zip(clip_gradients,trainable_params),\n",
    "                                                global_step = self.global_step)\n",
    "        \n",
    "    def save(self,sess,path,var_list=None,global_step=None):\n",
    "        saver = tf.train.Saver(var_list)\n",
    "        \n",
    "        save_path = saver.save(sess,save_path=path,global_step=step)\n",
    "        print('model saved at ',save_path)\n",
    "        \n",
    "    def restore(self,sess,path,var_list=None):\n",
    "        saver = tf.train.Saver(var_list)\n",
    "        saver.restore(sess, save_path = path)\n",
    "        print('model restored from ',path)\n",
    "    \n",
    "    def build_encoder(self):\n",
    "        print('Building Encoder..')\n",
    "        with tf.variable_scope('encoder'):\n",
    "            self.encoder_cell = self.build_encoder_cell()\n",
    "            \n",
    "            #Instantiating pretrained embeddings\n",
    "            embedding_variable = tf.Variable(tf.constant(0.0, shape = [self.encoder_vocab_size, embedding_size]),trainable = False, name = 'embedding')\n",
    "                           \n",
    "            self.encoder_embedding_placeholder = tf.placeholder(tf.float32, shape=[self.encoder_vocab_size,embedding_size], name = 'embedding_placeholder' )\n",
    "            self.encoder_embeddings = embedding_variable.assign(self.encoder_embedding_placeholder)\n",
    "            self.encoder_inputs_embedded=tf.nn.embedding_lookup(self.encoder_embeddings,self.encoder_inputs)\n",
    "            \n",
    "    \n",
    "            #instantiating dense layer\n",
    "            input_layer = Dense(self.hidden_units, dtype = self.dtype, name = 'input_projection')\n",
    "            #passing the embedding through dense layer\n",
    "            self.encoder_inputs_embedded = input_layer(self.encoder_inputs_embedded)\n",
    "            \n",
    "            #Encode input sequences into context vectors\n",
    "            #encoder_outputs: [batch_size, max_time_step, cell_output_size]\n",
    "            #encoder_state: [batch_size,cell_output_size]\n",
    "            self.encoder_outputs, self.encoder_last_state = tf.nn.dynamic_rnn(cell = self.encoder_cell,\n",
    "                                                                               inputs=self.encoder_inputs_embedded,\n",
    "                                                                               sequence_length=self.encoder_inputs_length,\n",
    "                                                                               dtype=self.dtype,\n",
    "                                                                               time_major=False)\n",
    "            \n",
    "            '''\n",
    "            init = tf.global_variables_initializer()\n",
    "            with tf.Session() as sess:\n",
    "                sess.run(init)\n",
    "                enc_outputs,enc_laststate=sess.run([self.encoder_outputs,self.encoder_last_state], \n",
    "                                                   feed_dict={self.encoder_embedding_placeholder:emb ,\n",
    "                                                              self.encoder_inputs:eInput, \n",
    "                                                              self.encoder_inputs_length: eLengths })\n",
    "                print('encoder Outputs:',enc_outputs.shape)\n",
    "                print(enc_outputs)\n",
    "                print()\n",
    "                print('Encoder last state:',len(enc_laststate))\n",
    "                print(enc_laststate)\n",
    "            '''\n",
    "        \n",
    "\n",
    "    def build_decoder(self):\n",
    "        print('Building decoder and attention...')\n",
    "        with tf.variable_scope('decoder'):\n",
    "                \n",
    "            #Recheck this code\n",
    "            self.decoder_cell,self.decoder_initial_state = self.build_decoder_cell()\n",
    "            \n",
    "            #Instantiating pretrained embeddings\n",
    "            embedding_variable = tf.Variable(tf.constant(0.0, shape = [self.decoder_vocab_size, embedding_size]),trainable = False, name = 'embedding')\n",
    "\n",
    "            self.decoder_embedding_placeholder = tf.placeholder(tf.float32, shape=[self.decoder_vocab_size,embedding_size], name = 'embedding_placeholder' )\n",
    "            self.decoder_embeddings = embedding_variable.assign(self.decoder_embedding_placeholder)\n",
    "\n",
    "                \n",
    "            #instantiating dense layer --> DOUBT\n",
    "            input_layer = Dense(self.hidden_units, dtype = self.dtype, name = 'input_projection')\n",
    "                \n",
    "            #Output projection layer to convert cell outputs to logits --> DOUBT\n",
    "            output_layer = Dense(self.decoder_vocab_size,name = \"output_projection\")\n",
    "                \n",
    "            if self.mode == 'train':\n",
    "                #decoder_inputs_embedded: [batch_size,max_time_step,embedding_size]\n",
    "                self.decoder_inputs_embedded = tf.nn.embedding_lookup(self.decoder_embeddings,\n",
    "                                                                           self.decoder_inputs)\n",
    "                    \n",
    "                #Embedded inputs going through projection layer\n",
    "                self.decoder_inputs_embedded=input_layer(self.decoder_inputs_embedded)\n",
    "                    \n",
    "                #Helper to feed inputs for training: read inputs from dense ground truth vectors\n",
    "                training_helper = seq2seq.TrainingHelper(inputs = self.decoder_inputs_embedded,\n",
    "                                                            sequence_length=self.decoder_inputs_length,\n",
    "                                                            time_major=False,\n",
    "                                                            name='training_helper')\n",
    "                training_decoder = seq2seq.BasicDecoder(cell=self.decoder_cell,\n",
    "                                                           helper = training_helper,\n",
    "                                                           initial_state = self.decoder_initial_state,\n",
    "                                                           output_layer = output_layer)\n",
    "                                                           #output_layer = output_layer\n",
    "                    \n",
    "                #Maximum decoder time_steps in current batch\n",
    "                max_decoder_length = tf.reduce_max(self.decoder_inputs_length)\n",
    "                \n",
    "                # decoder_outputs_train: BasicDecoderOutput\n",
    "                #                        namedtuple(rnn_outputs, sample_id)\n",
    "                # decoder_outputs_train.rnn_output: [batch_size, max_time_step + 1, num_decoder_symbols] if output_time_major=False\n",
    "                #                                   [max_time_step + 1, batch_size, num_decoder_symbols] if output_time_major=True\n",
    "                # decoder_outputs_train.sample_id: [batch_size], tf.int32\n",
    "                \n",
    "                (self.decoder_outputs_train, self.decoder_last_state_train,\n",
    "                self.decoder_outputs_length_train) = (seq2seq.dynamic_decode(decoder=training_decoder,\n",
    "                                                                            output_time_major=False,\n",
    "                                                                            impute_finished=True,\n",
    "                                                                            maximum_iterations=max_decoder_length))\n",
    "                \n",
    "                    \n",
    "\n",
    "                \n",
    "    \n",
    "                    \n",
    "                \n",
    "                # More efficient to do the projection on the batch-time-concatenated tensor\n",
    "                # logits_train: [batch_size, max_time_step + 1, num_decoder_symbols]\n",
    "                # self.decoder_logits_train = output_layer(self.decoder_outputs_train.rnn_output)\n",
    "                self.decoder_logits_train = tf.identity(self.decoder_outputs_train.rnn_output) \n",
    "                # Use argmax to extract decoder symbols to emit\n",
    "                self.decoder_pred_train = tf.argmax(self.decoder_logits_train, axis=-1,\n",
    "                                                        name='decoder_pred_train')\n",
    "                    \n",
    "                # masks: masking for valid and padded time steps, [batch_size, max_time_step + 1]\n",
    "                masks = tf.sequence_mask(lengths=self.decoder_inputs_length, \n",
    "                                         maxlen=max_decoder_length, dtype=self.dtype, name='masks')\n",
    "\n",
    "                # Computes per word average cross-entropy over a batch\n",
    "                # Internally calls 'nn_ops.sparse_softmax_cross_entropy_with_logits' by default\n",
    "                self.loss = seq2seq.sequence_loss(logits=self.decoder_logits_train, \n",
    "                                                  targets=self.decoder_targets,\n",
    "                                                  weights=masks,\n",
    "                                                  average_across_timesteps=True,\n",
    "                                                  average_across_batch=True,)\n",
    "                # Training summary for the current batch_loss\n",
    "                    \n",
    "                # Training summary for the current batch_loss\n",
    "                tf.summary.scalar('loss', self.loss)\n",
    "\n",
    "                # Contruct graphs for minimizing loss\n",
    "                self.init_optimizer()\n",
    "                \n",
    "                #return self.decoder_logits_train,self.decoder_pred_train\n",
    "                #The above return can be removed from comment to test.\n",
    "                \n",
    "            \n",
    "            #When decoding. The output of every time step will go as an input\n",
    "            #to the next time step. Similar to a language model.\n",
    "            elif self.mode == 'decode':\n",
    "                \n",
    "                #Must be of the size [batch_size,] --> int32 vector\n",
    "                start_tokens = tf.ones([self.batch_size,],tf.int32)*go_index\n",
    "                #Must be scalar to be passed to greedyEmbeddingHelper\n",
    "                end_token = eos_index\n",
    "                \n",
    "                def embed_and_input_proj(inputs):\n",
    "                    return input_layer(tf.nn.embedding_lookup(self.decoder_embeddings,inputs))\n",
    "                \n",
    "                \n",
    "                if not self.use_beamsearch_decode:\n",
    "                    #Helper to feed inputs for greedy decoding : Uses argmax of the output\n",
    "                    decoding_helper = seq2seq.GreedyEmbeddingHelper(start_tokens=start_tokens,\n",
    "                                                                   end_token=end_token,\n",
    "                                                                   embedding=embed_and_input_proj)\n",
    "                    \n",
    "                    #Basic decoder performs greedy decoding at each time step\n",
    "                    print(\"Building greedy decoder..\")\n",
    "                    inference_decoder = seq2seq.BasicDecoder(cell=self.decoder_cell,\n",
    "                                                            helper = decoding_helper,\n",
    "                                                            initial_state=self.decoder_initial_state,\n",
    "                                                            output_layer=output_layer)\n",
    "                else:\n",
    "                    #Less greedy approach since we see outputs from a few paths.\n",
    "                    print(\"building beamsearch decoder..\")\n",
    "                    inference_decoder = beam_search_decoder.BeamSearchDecoder(cell=self.decoder_cell,\n",
    "                                                                             embedding=embed_and_input_proj,\n",
    "                                                                             start_tokens=start_tokens,\n",
    "                                                                             end_token=end_token,\n",
    "                                                                             initial_state=self.decoder_initial_state,\n",
    "                                                                             beam_width=self.beam_width,\n",
    "                                                                             output_layer=output_layer)\n",
    "                    \n",
    "                (self.decoder_outputs_decode, self.decoder_last_state_decode,\n",
    "                self.decoder_outputs_length_decode) = (seq2seq.dynamic_decode(\n",
    "                decoder=inference_decoder,\n",
    "                output_time_major=False,\n",
    "                #impute_finished=True, #Could be an error\n",
    "                maximum_iterations=self.max_decode_step))\n",
    "                \n",
    "                if not self.use_beamsearch_decode:\n",
    "                    self.decoder_pred_decode = tf.expand_dims(self.decoder_outputs_decode.sample_id,-1)\n",
    "                else:\n",
    "                    self.decoder_pred_decode = self.decoder_outputs_decode.predicted_ids\n",
    "                    \n",
    "                return self.decoder_pred_decode\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model..\n",
      "building model..\n",
      "Building Encoder..\n",
      "Building decoder and attention...\n",
      "using beamsearch..\n",
      "building beamsearch decoder..\n",
      "encoder Outputs: (6, 100, 64)\n",
      "\n",
      "\n",
      "Decoder predicted: (6, 18, 3)\n",
      "[[[203508 203508 203508]\n",
      "  [327153 327153 327153]\n",
      "  [218513 218513 218513]\n",
      "  [131971 131971 131971]\n",
      "  [108134 108134 108134]\n",
      "  [142068 142068 142068]\n",
      "  [335172 335172 335172]\n",
      "  [267093 267093 267093]\n",
      "  [277221 277221 277221]\n",
      "  [ 73386  73386  73386]\n",
      "  [251818 251818 251818]\n",
      "  [128052 128052 128052]\n",
      "  [318215 318215 318215]\n",
      "  [254936 254936 254936]\n",
      "  [214997 214997 214997]\n",
      "  [ 73386  73386  73386]\n",
      "  [238380 238380 238380]\n",
      "  [363439 125410 361897]]\n",
      "\n",
      " [[392954 392954 392954]\n",
      "  [245144 245144 245144]\n",
      "  [ 52425  52425  52425]\n",
      "  [ 69651  69651  69651]\n",
      "  [  1174   1174   1174]\n",
      "  [381634 381634 381634]\n",
      "  [393803 393803 393803]\n",
      "  [395522 395522 395522]\n",
      "  [203914 203914 203914]\n",
      "  [145147 145147 145147]\n",
      "  [206348 206348 206348]\n",
      "  [306376 306376 306376]\n",
      "  [115473 115473 115473]\n",
      "  [280450 280450 280450]\n",
      "  [332299 332299 332299]\n",
      "  [342391 342391 342391]\n",
      "  [137244 137244 133154]\n",
      "  [115609 326213 164399]]\n",
      "\n",
      " [[154385 154385 154385]\n",
      "  [ 64818  64818  64818]\n",
      "  [137244 137244 137244]\n",
      "  [377439 377439 377439]\n",
      "  [377439 377439 377439]\n",
      "  [115609 115609 115609]\n",
      "  [245144 245144 245144]\n",
      "  [ 52425  52425  52425]\n",
      "  [115609 115609 115609]\n",
      "  [186723 186723 186723]\n",
      "  [331436 331436 331436]\n",
      "  [ 31097  31097  31097]\n",
      "  [322313 322313 322313]\n",
      "  [325585 325585 325585]\n",
      "  [284949 284949 284949]\n",
      "  [ 12803  12803  12803]\n",
      "  [115609 115609 115609]\n",
      "  [228373 306910 154018]]\n",
      "\n",
      " [[243821 243821 243821]\n",
      "  [ 81255  81255  81255]\n",
      "  [ 58429  58429  58429]\n",
      "  [125135 125135 125135]\n",
      "  [377439 377439 377439]\n",
      "  [252455 252455 252455]\n",
      "  [379324 379324 379324]\n",
      "  [231014 231014 231014]\n",
      "  [ 30528  30528  30528]\n",
      "  [368530 368530 368530]\n",
      "  [388303 388303 388303]\n",
      "  [ 32923  32923  32923]\n",
      "  [295117 295117 295117]\n",
      "  [  5478   5478   5478]\n",
      "  [156263 156263 156263]\n",
      "  [286675 286675 286675]\n",
      "  [388501 388501 388501]\n",
      "  [115598 377439 138090]]\n",
      "\n",
      " [[259688 259688 259688]\n",
      "  [245144 245144 245144]\n",
      "  [ 52425  52425  52425]\n",
      "  [ 94027  94027  94027]\n",
      "  [188598 188598 188598]\n",
      "  [ 20577  20577  20577]\n",
      "  [259688 259688 259688]\n",
      "  [247807 247807 247807]\n",
      "  [ 62587  62587  62587]\n",
      "  [ 19920  19920  19920]\n",
      "  [ 52545  52545  52545]\n",
      "  [115609 115609 115609]\n",
      "  [396197 396197 396197]\n",
      "  [242102 242102 242102]\n",
      "  [228062 228062 228062]\n",
      "  [ 25568  25568  25568]\n",
      "  [ 28501  28501  28501]\n",
      "  [ 28501 186600 397929]]\n",
      "\n",
      " [[127163 127163 127163]\n",
      "  [387851 387851 387851]\n",
      "  [341933 341933 341933]\n",
      "  [261124 261124 261124]\n",
      "  [344861 344861 344861]\n",
      "  [115473 115473 115473]\n",
      "  [ 89833  89833  89833]\n",
      "  [115609 115609 115609]\n",
      "  [115609 115609 115609]\n",
      "  [287858 287858 287858]\n",
      "  [115609 115609 115609]\n",
      "  [315302 315302 315302]\n",
      "  [377439 377439 377439]\n",
      "  [230990 230990 230990]\n",
      "  [306991 306991 306991]\n",
      "  [ 64659  64659  64659]\n",
      "  [320642 320642 320642]\n",
      "  [329457 254936 320642]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing Seq2Seq\n",
    "reset_graph()\n",
    "\n",
    "config ={'cell_type': 'lstm',\n",
    "         'hidden_units': 64 ,\n",
    "         'depth': 2,\n",
    "         'attention_type': 'bahdanou',\n",
    "          'embedding_size': 50,\n",
    "           'use_residual': True,\n",
    "          'attn_input_feeding': False ,\n",
    "           'use_dropout': True,\n",
    "        'dropout_rate' : 0.3,\n",
    "        'optimizer' : 'Adam',\n",
    "        'learning_rate' : 0.001,\n",
    "        'max_gradient_norm': 1.0,\n",
    "        'use_float16': False,\n",
    "        'beam_width': 3,\n",
    "        'max_decode_step': 18 }\n",
    "\n",
    "\n",
    "\n",
    "phase = 'decode'\n",
    "obj = Seq2SeqModel(config,phase)\n",
    "obj.build_model()\n",
    "obj.build_encoder()\n",
    "\n",
    "if(phase=='train'):\n",
    "    dec_logits,dec_argmax = obj.build_decoder()\n",
    "    \n",
    "if(phase=='decode'):\n",
    "    decoder_predicted = obj.build_decoder()\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    eie,enc_outputs,enc_laststate=sess.run([obj.encoder_inputs_embedded,obj.encoder_outputs,obj.encoder_last_state], \n",
    "                                                   feed_dict={obj.encoder_embedding_placeholder:emb ,\n",
    "                                                              obj.encoder_inputs:eInput, \n",
    "                                                              obj.encoder_inputs_length: eLengths,\n",
    "                                                              obj.keep_prob_placeholder : 0.3 })\n",
    "    \n",
    "    if(phase=='train'):\n",
    "        dec_logits,dec_argmax = sess.run([dec_logits,dec_argmax],feed_dict={obj.decoder_embedding_placeholder:emb ,\n",
    "                                                                  obj.decoder_inputs_length: dLengths,\n",
    "                                                                  obj.decoder_targets:dOutput,\n",
    "                                                                  obj.decoder_inputs: dInput,\n",
    "                                                                  obj.encoder_embedding_placeholder:emb,\n",
    "                                                                  obj.encoder_inputs:eInput, \n",
    "                                                                  obj.encoder_inputs_length: eLengths,\n",
    "                                                                  obj.keep_prob_placeholder : 0.3} )\n",
    "    \n",
    "    if(phase=='decode'):\n",
    "        decoder_predicted = sess.run(decoder_predicted,feed_dict={obj.decoder_embedding_placeholder:emb,\n",
    "                                                                  obj.encoder_embedding_placeholder:emb,\n",
    "                                                                  obj.encoder_inputs:eInput, \n",
    "                                                                  obj.encoder_inputs_length: eLengths,\n",
    "                                                                  obj.keep_prob_placeholder : 0.3})\n",
    "\n",
    "    print('encoder Outputs:',enc_outputs.shape)\n",
    "    #print(enc_outputs[0])\n",
    "    print()\n",
    "    #print('Encoder last state:',len(enc_laststate))\n",
    "    #print(enc_laststate)\n",
    "    print()\n",
    "    \n",
    "    if(phase=='train'):\n",
    "        print(\"Decoder Logits:\",dec_logits.shape)\n",
    "        print(dec_logits)\n",
    "        print()\n",
    "        print()\n",
    "        print(\"Decoder Argmax:\",dec_argmax.shape)\n",
    "        print()\n",
    "        print(dec_argmax)\n",
    "        print()\n",
    "        \n",
    "    if(phase=='decode'):\n",
    "        print(\"Decoder predicted:\",decoder_predicted.shape)\n",
    "        print(decoder_predicted)\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
