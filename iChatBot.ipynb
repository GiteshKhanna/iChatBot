{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "\n",
    "from tensorflow.python.ops.rnn_cell import GRUCell\n",
    "from tensorflow.python.ops.rnn_cell import LSTMCell\n",
    "from tensorflow.python.ops.rnn_cell import MultiRNNCell\n",
    "from tensorflow.python.ops.rnn_cell import DropoutWrapper, ResidualWrapper\n",
    "\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.util import nest\n",
    "\n",
    "\n",
    "from tensorflow.contrib.seq2seq.python.ops import attention_wrapper\n",
    "from tensorflow.contrib.seq2seq.python.ops import beam_search_decoder\n",
    "\n",
    "from preprocess import *\n",
    "from loading_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resetter\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding parameters\n",
    "embedding_size = 50\n",
    "\n",
    "#data parameters\n",
    "eMax_allowed_length = 100\n",
    "dMax_allowed_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching data\n",
    "#default directory: 'data/data_10.csv'\n",
    "X,Y= read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching glove vectors\n",
    "#default directory: \"./glove.6B.50d.txt\"\n",
    "embedding_size = 50\n",
    "wi,iw,wv = read_glove_vecs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding extra tokens to glove dictionary\n",
    "go_index,eos_index,unk_index = add_extra_to_dict(wi,iw,wv,embedding_size)\n",
    "emb = map_dict_to_list(iw,wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 19, 13, 18, 11, 18]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocessing data\n",
    "#Mapping each word in a sentence to its glove index\n",
    "eInput,eLengths = fit_encoder_text(data= X[1:],word_to_index = wi,max_allowed_seq_length = eMax_allowed_length)\n",
    "dInput,dOutput,dLengths = fit_decoder_text(data= Y[1:],word_to_index = wi,max_allowed_seq_length = dMax_allowed_length)\n",
    "dLengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel():\n",
    "    def __init__(self,config,mode):\n",
    "        assert mode.lower() in ['train','decode']\n",
    "        \n",
    "        self.mode = mode.lower()\n",
    "        \n",
    "        #num_encoder_symbols and num_decoder_symbols\n",
    "        self.encoder_vocab_size = 400003\n",
    "        self.decoder_vocab_size = 400003\n",
    "\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        \n",
    "        self.cell_type = config['cell_type']\n",
    "        self.hidden_units = config['hidden_units']\n",
    "        self.depth = config['depth']\n",
    "        self.attention_type = config['attention_type']\n",
    "        self.embedding_size = config['embedding_size']\n",
    "        \n",
    "        self.use_residual = config['use_residual']\n",
    "        self.attn_input_feeding = config['attn_input_feeding']\n",
    "        self.use_dropout = config['use_dropout']\n",
    "        self.keep_prob = 1.0 - config['dropout_rate']\n",
    "        \n",
    "        self.optimizer = config['optimizer']\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.max_gradient_norm = config['max_gradient_norm']\n",
    "        self.global_step = tf.Variable(0, trainable = False, name = 'global_step')\n",
    "        self.global_epoch_step = tf.Variable(0,trainable=False, name = \"global_epoch_step\")\n",
    "        self.global_epoch_step_op= tf.assign(self.global_epoch_step,self.global_epoch_step+1)\n",
    "        \n",
    "        self.dtype = tf.float16 if config['use_float16'] else tf.float32\n",
    "        self.keep_prob_placeholder = tf.placeholder(self.dtype, shape=[], name = 'keep_prob')\n",
    "        \n",
    "        self.use_beamsearch_decode = False\n",
    "        if self.mode == 'decode':\n",
    "            self.beam_width = config['beam_width']\n",
    "            self.use_beamsearch_decode = True if self.beam_width > 1 else False\n",
    "            self.max_decode_step = config['max_decode_step']\n",
    "        \n",
    "        self.build_model()\n",
    "    \n",
    "    def build_model(self):\n",
    "            print('building model..')\n",
    "\n",
    "            #building encoder and decoder networks\n",
    "            self.init_placeholders()\n",
    "            '''\n",
    "            self.build_encoder()\n",
    "            self.build_decoder()\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "            '''    \n",
    "    def init_placeholders(self):\n",
    "            #encoder inputs: [batch_size, max_time_steps]\n",
    "            self.encoder_inputs = tf.placeholder(dtype = tf.int32, shape = (None,None), name = 'encoder_inputs')\n",
    "            #encoder_inputs_length: [batch_size]\n",
    "            self.encoder_inputs_length = tf.placeholder(dtype=tf.int32, shape=(None,) , name = 'encoder_inputs_length')\n",
    "            \n",
    "            #get dynamic batch_size\n",
    "            self.batch_size = tf.shape(self.encoder_inputs)[0]\n",
    "            \n",
    "            if(self.mode=='train'):\n",
    "                \n",
    "                #decoder_inputs: [batch_size,max_time_steps]\n",
    "                self.decoder_inputs = tf.placeholder(dtype=tf.int32,shape=(None,None), name ='decoder_inputs')\n",
    "                #decoder_inputs_length: [batch_size]\n",
    "                self.decoder_inputs_length = tf.placeholder(dtype=tf.int32, shape=(None,), name='decoder_inputs_length')\n",
    "                \n",
    "                self.decoder_targets = tf.placeholder(dtype=tf.int32,shape=(None,None), name ='decoder_targets')\n",
    "                \n",
    "                '''\n",
    "                #No need, already preprocessed\n",
    "                decoder_start_token=tf.ones(shape=[self.batch_size,1], dtype=tf.int32)*data_utils.start_token\n",
    "                \n",
    "                decoder_end_token=tf.ones(shape=[self.batch_size,1], dtype=tf.int32)*data_utils.end_token\n",
    "                '''\n",
    "                \n",
    "    def build_single_cell(self):\n",
    "        cell_type = LSTMCell\n",
    "        if(self.cell_type.lower() == 'gru'):\n",
    "            cell_type = GRUCell\n",
    "        cell = cell_type(self.hidden_units)\n",
    "        \n",
    "        if self.use_dropout:\n",
    "            cell = DropoutWrapper(cell,dtype=self.dtype,\n",
    "                                 output_keep_prob = self.keep_prob_placeholder)\n",
    "            \n",
    "        if self.use_residual:\n",
    "            cell = ResidualWrapper(cell)\n",
    "            \n",
    "        return cell\n",
    "\n",
    "    def build_encoder_cell (self):\n",
    "        return MultiRNNCell([self.build_single_cell() for i in range(self.depth)])\n",
    "    \n",
    "    def build_decoder_cell(self):\n",
    "        encoder_outputs = self.encoder_outputs\n",
    "        encoder_last_state = self.encoder_last_state\n",
    "        encoder_inputs_length = self.encoder_inputs_length\n",
    "        \n",
    "        if self.use_beamsearch_decode:\n",
    "            print('using beamsearch..')\n",
    "            encoder_outputs = seq2seq.tile_batch(self.encoder_outputs,\n",
    "                                                 multiplier=self.beam_width)\n",
    "            encoder_last_state = nest.map_structure( lambda s: seq2seq.tile_batch(s,self.beam_width),\n",
    "                                                   self.encoder_last_state)\n",
    "            encoder_inputs_length = seq2seq.tile_batch(self.encoder_inputs_length,\n",
    "                                                       multiplier=self.beam_width)\n",
    "            \n",
    "            \n",
    "        #Building attention mechanism: Default Bahdanau\n",
    "        #'Bahdanau' style attention\n",
    "        self.attention_mechanism = attention_wrapper.BahdanauAttention(\n",
    "        num_units=self.hidden_units, memory=encoder_outputs,\n",
    "        memory_sequence_length=encoder_inputs_length,\n",
    "        name='BahdanauAttention')\n",
    "        \n",
    "        # 'Luong' style attention:\n",
    "        if self.attention_type.lower() == 'luong':\n",
    "            self.attention_mechanism = attention_wrapper.LuongAttention(\n",
    "            num_units = self.hidden_units, memory=encoder_outputs,\n",
    "            memory_sequence_length=encoder_inputs_length,\n",
    "            name='LuongAttention')\n",
    "                \n",
    "        #Building decoder_cell\n",
    "        self.decoder_cell_list = [self.build_single_cell() for i in range(self.depth)]\n",
    "        decoder_initial_state = encoder_last_state\n",
    "        \n",
    "        def attn_decoder_input_fn(inputs,attention):\n",
    "            if not self.attn_input_feeding:\n",
    "                return inputs\n",
    "            \n",
    "            _input_layer = Dense(self.hidden_units,dtype = self.dtype,\n",
    "                                name = 'attn_input_feeding')\n",
    "            return _input_layer(array_ops.concat([inputs,attention],-1))\n",
    "        \n",
    "        self.decoder_cell_list[-1] = attention_wrapper.AttentionWrapper(\n",
    "        cell = self.decoder_cell_list[-1],\n",
    "        attention_mechanism=self.attention_mechanism,\n",
    "        attention_layer_size=self.hidden_units,\n",
    "        cell_input_fn=attn_decoder_input_fn,\n",
    "        initial_cell_state=encoder_last_state[-1],\n",
    "        alignment_history=False,\n",
    "        name='Attention_Wrapper')\n",
    "        \n",
    "        # Encoder last state must be compatible with AttentionWrapper\n",
    "        #Attentionwrapper.zero_state is used for the purpose\n",
    "        \n",
    "        batch_size = self.batch_size if not self.use_beamsearch_decode else self.batch_size*self.beam_width\n",
    "        initial_state = [state for state in encoder_last_state]\n",
    "        \n",
    "        initial_state[-1]= self.decoder_cell_list[-1].zero_state(\n",
    "        batch_size = batch_size, dtype=self.dtype)\n",
    "        decoder_initial_state = tuple(initial_state)\n",
    "        \n",
    "        return MultiRNNCell(self.decoder_cell_list),decoder_initial_state\n",
    "        \n",
    "    \n",
    "    def init_optimizer(self):\n",
    "        print(\"Setting optimizer..\")\n",
    "        #Gradients and SGD update operaton for training the model\n",
    "        trainable_params = tf.trainable_variables()\n",
    "        if self.optimizer.lower() == 'adadelta':\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate = self.learning_rate)\n",
    "        elif self.optimizer.lower() == 'adam':\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate = self.learning_rate)\n",
    "        elif self.optimizer.lower() == 'rmsprop':\n",
    "            self.opt = tf.train.RMSPropOptimizer(learning_rate = self.learning_rate)\n",
    "        else:\n",
    "            self.opt = tf.train.GradientDescentOptimizer(learning_rate = self.learning_rate)\n",
    "            \n",
    "        \n",
    "        #Compute gradients of loss w.r.t all trainable variables\n",
    "        gradients = tf.gradients(self.loss,trainable_params)\n",
    "        \n",
    "        #Clip gradients of loss w.r.t all trainable variables\n",
    "        clip_gradients,_ = tf.clip_by_global_norm(gradients,self.max_gradient_norm)\n",
    "        \n",
    "        #Update the model\n",
    "        self.updates = self.opt.apply_gradients(zip(clip_gradients,trainable_params),\n",
    "                                                global_step = self.global_step)\n",
    "        \n",
    "    def save(self,sess,path,var_list=None,global_step=None):\n",
    "        saver = tf.train.Saver(var_list)\n",
    "        \n",
    "        save_path = saver.save(sess,save_path=path,global_step=step)\n",
    "        print('model saved at ',save_path)\n",
    "        \n",
    "    def restore(self,sess,path,var_list=None):\n",
    "        saver = tf.train.Saver(var_list)\n",
    "        saver.restore(sess, save_path = path)\n",
    "        print('model restored from ',path)\n",
    "    \n",
    "    def build_encoder(self):\n",
    "        print('Building Encoder..')\n",
    "        with tf.variable_scope('encoder'):\n",
    "            self.encoder_cell = self.build_encoder_cell()\n",
    "            \n",
    "            #Instantiating pretrained embeddings\n",
    "            embedding_variable = tf.Variable(tf.constant(0.0, shape = [self.encoder_vocab_size, embedding_size]),trainable = False, name = 'embedding')\n",
    "                           \n",
    "            self.encoder_embedding_placeholder = tf.placeholder(tf.float32, shape=[self.encoder_vocab_size,embedding_size], name = 'embedding_placeholder' )\n",
    "            self.encoder_embeddings = embedding_variable.assign(self.encoder_embedding_placeholder)\n",
    "            self.encoder_inputs_embedded=tf.nn.embedding_lookup(self.encoder_embeddings,self.encoder_inputs)\n",
    "            \n",
    "    \n",
    "            #instantiating dense layer\n",
    "            input_layer = Dense(self.hidden_units, dtype = self.dtype, name = 'input_projection')\n",
    "            #passing the embedding through dense layer\n",
    "            self.encoder_inputs_embedded = input_layer(self.encoder_inputs_embedded)\n",
    "            \n",
    "            #Encode input sequences into context vectors\n",
    "            #encoder_outputs: [batch_size, max_time_step, cell_output_size]\n",
    "            #encoder_state: [batch_size,cell_output_size]\n",
    "            self.encoder_outputs, self.encoder_last_state = tf.nn.dynamic_rnn(cell = self.encoder_cell,\n",
    "                                                                               inputs=self.encoder_inputs_embedded,\n",
    "                                                                               sequence_length=self.encoder_inputs_length,\n",
    "                                                                               dtype=self.dtype,\n",
    "                                                                               time_major=False)\n",
    "            \n",
    "            '''\n",
    "            init = tf.global_variables_initializer()\n",
    "            with tf.Session() as sess:\n",
    "                sess.run(init)\n",
    "                enc_outputs,enc_laststate=sess.run([self.encoder_outputs,self.encoder_last_state], \n",
    "                                                   feed_dict={self.encoder_embedding_placeholder:emb ,\n",
    "                                                              self.encoder_inputs:eInput, \n",
    "                                                              self.encoder_inputs_length: eLengths })\n",
    "                print('encoder Outputs:',enc_outputs.shape)\n",
    "                print(enc_outputs)\n",
    "                print()\n",
    "                print('Encoder last state:',len(enc_laststate))\n",
    "                print(enc_laststate)\n",
    "            '''\n",
    "        \n",
    "\n",
    "    def build_decoder(self):\n",
    "        print('Building decoder and attention...')\n",
    "        with tf.variable_scope('decoder'):\n",
    "                \n",
    "            #Recheck this code\n",
    "            self.decoder_cell,self.decoder_initial_state = self.build_decoder_cell()\n",
    "            \n",
    "            #Instantiating pretrained embeddings\n",
    "            embedding_variable = tf.Variable(tf.constant(0.0, shape = [self.decoder_vocab_size, embedding_size]),trainable = False, name = 'embedding')\n",
    "\n",
    "            self.decoder_embedding_placeholder = tf.placeholder(tf.float32, shape=[self.decoder_vocab_size,embedding_size], name = 'embedding_placeholder' )\n",
    "            self.decoder_embeddings = embedding_variable.assign(self.decoder_embedding_placeholder)\n",
    "\n",
    "                \n",
    "            #instantiating dense layer --> DOUBT\n",
    "            input_layer = Dense(self.hidden_units, dtype = self.dtype, name = 'input_projection')\n",
    "                \n",
    "            #Output projection layer to convert cell outputs to logits --> DOUBT\n",
    "            output_layer = Dense(self.decoder_vocab_size,name = \"output_projection\")\n",
    "                \n",
    "            if self.mode == 'train':\n",
    "                #decoder_inputs_embedded: [batch_size,max_time_step,embedding_size]\n",
    "                self.decoder_inputs_embedded = tf.nn.embedding_lookup(self.decoder_embeddings,\n",
    "                                                                           self.decoder_inputs)\n",
    "                    \n",
    "                #Embedded inputs going through projection layer\n",
    "                self.decoder_inputs_embedded=input_layer(self.decoder_inputs_embedded)\n",
    "                    \n",
    "                #Helper to feed inputs for training: read inputs from dense ground truth vectors\n",
    "                training_helper = seq2seq.TrainingHelper(inputs = self.decoder_inputs_embedded,\n",
    "                                                            sequence_length=self.decoder_inputs_length,\n",
    "                                                            time_major=False,\n",
    "                                                            name='training_helper')\n",
    "                training_decoder = seq2seq.BasicDecoder(cell=self.decoder_cell,\n",
    "                                                           helper = training_helper,\n",
    "                                                           initial_state = self.decoder_initial_state,\n",
    "                                                           output_layer = output_layer)\n",
    "                                                           #output_layer = output_layer\n",
    "                    \n",
    "                #Maximum decoder time_steps in current batch\n",
    "                max_decoder_length = tf.reduce_max(self.decoder_inputs_length)\n",
    "                \n",
    "                # decoder_outputs_train: BasicDecoderOutput\n",
    "                #                        namedtuple(rnn_outputs, sample_id)\n",
    "                # decoder_outputs_train.rnn_output: [batch_size, max_time_step + 1, num_decoder_symbols] if output_time_major=False\n",
    "                #                                   [max_time_step + 1, batch_size, num_decoder_symbols] if output_time_major=True\n",
    "                # decoder_outputs_train.sample_id: [batch_size], tf.int32\n",
    "                \n",
    "                (self.decoder_outputs_train, self.decoder_last_state_train,\n",
    "                self.decoder_outputs_length_train) = (seq2seq.dynamic_decode(decoder=training_decoder,\n",
    "                                                                            output_time_major=False,\n",
    "                                                                            impute_finished=True,\n",
    "                                                                            maximum_iterations=max_decoder_length))\n",
    "                \n",
    "                    \n",
    "\n",
    "                \n",
    "    \n",
    "                    \n",
    "                \n",
    "                # More efficient to do the projection on the batch-time-concatenated tensor\n",
    "                # logits_train: [batch_size, max_time_step + 1, num_decoder_symbols]\n",
    "                # self.decoder_logits_train = output_layer(self.decoder_outputs_train.rnn_output)\n",
    "                self.decoder_logits_train = tf.identity(self.decoder_outputs_train.rnn_output) \n",
    "                # Use argmax to extract decoder symbols to emit\n",
    "                self.decoder_pred_train = tf.argmax(self.decoder_logits_train, axis=-1,\n",
    "                                                        name='decoder_pred_train')\n",
    "                    \n",
    "                # masks: masking for valid and padded time steps, [batch_size, max_time_step + 1]\n",
    "                masks = tf.sequence_mask(lengths=self.decoder_inputs_length, \n",
    "                                         maxlen=max_decoder_length, dtype=self.dtype, name='masks')\n",
    "\n",
    "                # Computes per word average cross-entropy over a batch\n",
    "                # Internally calls 'nn_ops.sparse_softmax_cross_entropy_with_logits' by default\n",
    "                self.loss = seq2seq.sequence_loss(logits=self.decoder_logits_train, \n",
    "                                                  targets=self.decoder_targets,\n",
    "                                                  weights=masks,\n",
    "                                                  average_across_timesteps=True,\n",
    "                                                  average_across_batch=True,)\n",
    "                # Training summary for the current batch_loss\n",
    "                    \n",
    "                # Training summary for the current batch_loss\n",
    "                tf.summary.scalar('loss', self.loss)\n",
    "\n",
    "                # Contruct graphs for minimizing loss\n",
    "                self.init_optimizer()\n",
    "                \n",
    "                return self.decoder_logits_train,self.decoder_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model..\n",
      "building model..\n",
      "Building Encoder..\n",
      "Building decoder and attention...\n",
      "Setting optimizer..\n",
      "encoder Outputs: (6, 100, 64)\n",
      "\n",
      "\n",
      "Decoder Logits: (6, 19, 400003)\n",
      "[[[ 0.00830366 -0.00013244  0.01428052 ... -0.00040389 -0.00327311\n",
      "   -0.01578742]\n",
      "  [ 0.00340551  0.01874045 -0.00704149 ... -0.00994961 -0.0146252\n",
      "   -0.00600149]\n",
      "  [ 0.00175845 -0.00871079 -0.00953527 ...  0.00163392 -0.0101069\n",
      "   -0.00013468]\n",
      "  ...\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]]\n",
      "\n",
      " [[ 0.0042647   0.00817887  0.00539594 ...  0.00307214  0.00921863\n",
      "   -0.0077023 ]\n",
      "  [ 0.01016874 -0.01356194 -0.00724075 ...  0.00473735 -0.00321013\n",
      "    0.00054495]\n",
      "  [ 0.00319019  0.01216287 -0.00452725 ... -0.0010238  -0.00118944\n",
      "   -0.00666791]\n",
      "  ...\n",
      "  [ 0.00158425 -0.0217423  -0.01902442 ...  0.02042277  0.01352476\n",
      "    0.00308334]\n",
      "  [ 0.00799268 -0.01413302 -0.02295904 ...  0.00884772 -0.00560004\n",
      "    0.0076876 ]\n",
      "  [ 0.00147231 -0.00891685 -0.0154519  ...  0.01315692  0.01756607\n",
      "   -0.01013753]]\n",
      "\n",
      " [[ 0.01410158 -0.0003669   0.01419454 ... -0.00482228  0.00538553\n",
      "   -0.01746879]\n",
      "  [ 0.00276557 -0.00614934 -0.00666494 ... -0.00840802 -0.01023695\n",
      "   -0.00244921]\n",
      "  [ 0.0038082  -0.00433688 -0.00218373 ...  0.00399056 -0.00217673\n",
      "   -0.00394241]\n",
      "  ...\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]]\n",
      "\n",
      " [[ 0.00832258  0.01208067  0.00757715 ...  0.0056214  -0.00360467\n",
      "   -0.02195118]\n",
      "  [ 0.00114436  0.02480904 -0.00625005 ... -0.00109537 -0.01020392\n",
      "   -0.00774251]\n",
      "  [-0.00254797  0.002634   -0.00882603 ... -0.00613175 -0.01422875\n",
      "   -0.01259723]\n",
      "  ...\n",
      "  [ 0.00040153  0.0003333  -0.01837507 ... -0.00358353  0.00309419\n",
      "    0.01269612]\n",
      "  [ 0.00692594 -0.01409193 -0.00186791 ...  0.01263871  0.01036813\n",
      "   -0.00331246]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]]\n",
      "\n",
      " [[ 0.01369364  0.0056631   0.00657818 ... -0.00269158  0.0062912\n",
      "   -0.0204118 ]\n",
      "  [-0.00030746 -0.00213815 -0.00207444 ... -0.0017043  -0.00361255\n",
      "   -0.00724258]\n",
      "  [ 0.00875001 -0.00496223 -0.0135471  ... -0.00235874 -0.00826604\n",
      "   -0.00936596]\n",
      "  ...\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]]\n",
      "\n",
      " [[ 0.01907077 -0.00929181  0.02263741 ... -0.00276563 -0.01179837\n",
      "   -0.0421718 ]\n",
      "  [ 0.00935011  0.0043329   0.00509977 ... -0.01689139 -0.01747497\n",
      "   -0.02304435]\n",
      "  [ 0.0153145  -0.00993915 -0.00552829 ... -0.0012414  -0.01444841\n",
      "   -0.02635538]\n",
      "  ...\n",
      "  [ 0.02550133 -0.01285161 -0.01520585 ...  0.00535577 -0.00824458\n",
      "   -0.00291612]\n",
      "  [-0.00452905 -0.02227887 -0.00263988 ...  0.01606905  0.00087754\n",
      "   -0.01117991]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      "Decoder Argmax: (6, 19)\n",
      "\n",
      "[[277261 140351 308928  33550 114173 338499  11178 155611      0      0\n",
      "       0      0      0      0      0      0      0      0      0]\n",
      " [373199 256508 326244 320420  82258  76900  96037  48974 249641 314086\n",
      "  148161 152923 397388 262492  33728 165689 290738 109999 285866]\n",
      " [277261  97142  98465 364996 230297 320482 114292 337948 331048 331048\n",
      "  250987  45118 180445      0      0      0      0      0      0]\n",
      " [378020  47850 291228 267295  47850 260178 275971 292991 337948 317433\n",
      "  332300 122442 352880  27252 344982 114173 223971 244484      0]\n",
      " [270251 332300 183836  95482 140066 105877 260178 206940  33728  71645\n",
      "   65295      0      0      0      0      0      0      0      0]\n",
      " [277261 286244  16932 308928 332300 331048 332300 207165 146870  88313\n",
      "  169952 125965 223219 210102 141480    486 194108 246465      0]]\n"
     ]
    }
   ],
   "source": [
    "#Testing Seq2Seq\n",
    "reset_graph()\n",
    "\n",
    "config ={'cell_type': 'lstm',\n",
    "         'hidden_units': 64 ,\n",
    "         'depth': 2,\n",
    "         'attention_type': 'bahdanou',\n",
    "          'embedding_size': 50,\n",
    "           'use_residual': True,\n",
    "          'attn_input_feeding': False ,\n",
    "           'use_dropout': True,\n",
    "        'dropout_rate' : 0.3,\n",
    "        'optimizer' : 'Adam',\n",
    "        'learning_rate' : 0.001,\n",
    "        'max_gradient_norm': 1.0,\n",
    "        'use_float16': False,\n",
    "        'beam_width': 3,\n",
    "        'max_decode_step': 18 }\n",
    "\n",
    "\n",
    "obj = Seq2SeqModel(config,'train')\n",
    "obj.build_model()\n",
    "obj.build_encoder()\n",
    "dec_logits,dec_argmax = obj.build_decoder()\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    eie,enc_outputs,enc_laststate=sess.run([obj.encoder_inputs_embedded,obj.encoder_outputs,obj.encoder_last_state], \n",
    "                                                   feed_dict={obj.encoder_embedding_placeholder:emb ,\n",
    "                                                              obj.encoder_inputs:eInput, \n",
    "                                                              obj.encoder_inputs_length: eLengths,\n",
    "                                                              obj.keep_prob_placeholder : 0.3 })\n",
    "    \n",
    "    dec_logits,dec_argmax = sess.run([dec_logits,dec_argmax],feed_dict={obj.decoder_embedding_placeholder:emb ,\n",
    "                                                              obj.decoder_inputs_length: dLengths,\n",
    "                                                              obj.decoder_targets:dOutput,\n",
    "                                                              obj.decoder_inputs: dInput,\n",
    "                                                              obj.encoder_embedding_placeholder:emb,\n",
    "                                                              obj.encoder_inputs:eInput, \n",
    "                                                              obj.encoder_inputs_length: eLengths,\n",
    "                                                              obj.keep_prob_placeholder : 0.3} )\n",
    "    \n",
    "    print('encoder Outputs:',enc_outputs.shape)\n",
    "    #print(enc_outputs[0])\n",
    "    print()\n",
    "    #print('Encoder last state:',len(enc_laststate))\n",
    "    #print(enc_laststate)\n",
    "    print()\n",
    "    print(\"Decoder Logits:\",dec_logits.shape)\n",
    "    print(dec_logits)\n",
    "    print()\n",
    "    print()\n",
    "    print(\"Decoder Argmax:\",dec_argmax.shape)\n",
    "    print()\n",
    "    print(dec_argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
